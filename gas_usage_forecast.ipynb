{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Description --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast future household gas consumption for central heating from previous household gas usage data, then create an optimisation app which utilises the usage pattern from the future forecast to provide the homeowner the ability to apply a cost threshold to their gas consumption in order to have more control and aid in keeping household energy bills down. \n",
    "\n",
    "Cast as time series problem but also consider converting to a supervised problem to allow the usage of general machine learning models and also possibly neural networks.\n",
    "\n",
    "Consider RMSE or MAE as possible performance metrics, reliant on data.\n",
    "\n",
    "Use yearly data 2014-2018, to predict the year 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Library Imports --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T05:58:02.104846Z",
     "start_time": "2021-09-27T05:57:51.592487Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from pmdarima.arima import auto_arima\n",
    "from dython.nominal import correlation_ratio, theils_u\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.stats import spearmanr, chi2_contingency, kruskal, anderson, normaltest, shapiro\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer, QuantileTransformer, PolynomialFeatures, MinMaxScaler\n",
    "from pyearth import Earth\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor, VotingRegressor, StackingRegressor, BaggingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression , RFE\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.distns import Exponential\n",
    "from hyperopt import atpe, tpe, fmin, hp, STATUS_OK, space_eval, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.optimize import differential_evolution\n",
    "from pygam.pygam import LinearGAM\n",
    "from itertools import combinations, product\n",
    "import joblib\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from kerastuner.tuners.bayesian import BayesianOptimization\n",
    "from kerastuner.tuners.hyperband import Hyperband\n",
    "from kerastuner.tuners.randomsearch import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import kerastuner as kt\n",
    "import os\n",
    "from pickle import dump, load\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_table\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "\"\"\"\n",
    "pandas==1.2.4\n",
    "numpy==1.19.2\n",
    "seaborn==0.11.1\n",
    "matplotlib==3.3.4\n",
    "statsmodels==0.12.2\n",
    "scikit-learn==0.24.1\n",
    "fbprophet==0.7.1\n",
    "pmdarima==1.8.2\n",
    "sweetviz==2.1.0\n",
    "dython==0.6.3\n",
    "scipy==1.6.2\n",
    "sklearn-contrib-py-earth==0.1.0\n",
    "xgboost==1.4.0\n",
    "lightgbm==3.1.1\n",
    "pycaret==2.2.0\n",
    "ngboost==0.3.11\n",
    "hyperopt==0.2.5\n",
    "pygam==0.8.0\n",
    "joblib==1.0.1\n",
    "tensorflow==2.3.0\n",
    "keras==2.4.3\n",
    "kerastuner==1.0.1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Functions --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:06:45.496878Z",
     "start_time": "2021-09-23T08:06:45.481103Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_data(data, file_name):\n",
    "    \"\"\" saves specified data as csv file with string filename \"\"\"\n",
    "    data.to_csv(file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T14:45:52.028509Z",
     "start_time": "2021-09-23T14:45:52.017142Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    \"\"\" loads csv data from string filename as pandas dataframe\"\"\"\n",
    "    features = pd.read_csv('gas_consumption_monthly_train_features.csv')\n",
    "    target = pd.read_csv('gas_consumption_monthly_train_target.csv')\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:06:45.530314Z",
     "start_time": "2021-09-23T08:06:45.519090Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    \"\"\" loads csv data from string filename as pandas dataframe\"\"\"\n",
    "    features = pd.read_csv('gas_consumption_monthly_test_features.csv')\n",
    "    target = pd.read_csv('gas_consumption_monthly_test_target.csv')\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:06:45.545596Z",
     "start_time": "2021-09-23T08:06:45.538993Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_clean_test_data():\n",
    "    \"\"\" loads clean test data... best_features\"\"\"\n",
    "    features, target = load_test_data()\n",
    "    # remove features\n",
    "    features = features[['month_sin', 'month_cos', 'exp_mean_ratio_outsidetemp_gas_used',\n",
    "                         'exp_mean_ratio_housetemp_gas_used', 'knn_local_knowledge']]\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:06:45.576738Z",
     "start_time": "2021-09-23T08:06:45.556726Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_clean_train_data():\n",
    "    \"\"\" loads clean train data.. best features, removed data\"\"\"\n",
    "    features, target = load_train_data()\n",
    "    # remove features\n",
    "    features = features[\n",
    "        ['month_sin','month_cos','exp_mean_ratio_outsidetemp_gas_used',\n",
    "         'exp_mean_ratio_housetemp_gas_used','knn_local_knowledge']] #,\n",
    "    # remove 2013 entries\n",
    "    features = features.iloc[2:,:].reset_index(drop=True)\n",
    "    target = target[2:].reset_index(drop=True)\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:06:45.591685Z",
     "start_time": "2021-09-23T08:06:45.580702Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_clean_train_data_lm():\n",
    "    \"\"\" loads clean train data.. best features, removed data\"\"\"\n",
    "    features, target = load_train_data()\n",
    "    # remove features\n",
    "    features = features[\n",
    "        ['month_sin','month_cos','exp_mean_ratio_outsidetemp_gas_used','exp_mean_ratio_outsidetemp_gas_used_bxcx',\n",
    "         'exp_mean_ratio_housetemp_gas_used','exp_mean_ratio_housetemp_gas_used_bxcx','knn_local_knowledge']] #'roll_mean_targ1'\n",
    "    # remove 2013 entries\n",
    "    features = features.iloc[2:,:].reset_index(drop=True)\n",
    "    target = target[2:].reset_index(drop=True)\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T05:58:13.458661Z",
     "start_time": "2021-09-27T05:58:13.438814Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost_of_gas(gas_used_m3, year):\n",
    "    \"\"\" \n",
    "    returns string for the cost of gas used \n",
    "    params:\n",
    "        gas_used_m3 = gas used in cubic meters\n",
    "        year = year you want the cost for, for correct coefficient \n",
    "    \"\"\"\n",
    "    if year == 2013:\n",
    "        year_coef = 9.76\n",
    "    elif year == 2014:\n",
    "        year_coef = 9.76\n",
    "    elif year == 2015:\n",
    "        year_coef = 9.74\n",
    "    elif year == 2016:\n",
    "        year_coef = 10.11\n",
    "    elif year == 2017:\n",
    "        year_coef = 10.1720\n",
    "    elif year == 2018:\n",
    "        year_coef = 11.1410\n",
    "    elif year == 2019:\n",
    "        year_coef = 11.3075 \n",
    "    avg_cost_kwh = 3.8\n",
    "    kwh = gas_used_m3 * year_coef\n",
    "    cost_pence = kwh * avg_cost_kwh\n",
    "    cost_pounds = np.round(cost_pence / 100, 2)\n",
    "    return f'£{cost_pounds}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:06:45.722342Z",
     "start_time": "2021-09-23T08:06:45.617220Z"
    }
   },
   "outputs": [],
   "source": [
    "def expanding_window_cv(features, target, model, fold_size=1, init_fold=12, scoring='rmse'):\n",
    "    \"\"\"\n",
    "    Performs cross validation for time series using an expanding window\n",
    "    params:\n",
    "        features = dataframe of features to perform cv on\n",
    "        target = target of features to perform cv on\n",
    "        model = model used to perform cv\n",
    "        fold_size = size of folds to train and test on top of init_fold (number of rows)\n",
    "        init_fold = size of first fold to train on (number of rows)\n",
    "        score = model scoring metric, one of [rmse, mae]\n",
    "    returns:\n",
    "        cv model scores\n",
    "    \"\"\"\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    r2_scores = []\n",
    "    preds = []\n",
    "    # inital cv fold\n",
    "    train_feats, train_targ, test_feats, test_targ = features.iloc[0:init_fold,:], target[0:init_fold], features.iloc[init_fold:init_fold+fold_size,:], target[init_fold:init_fold+fold_size]\n",
    "    model.fit(train_feats, train_targ)\n",
    "    cv_pred = model.predict(test_feats)\n",
    "    # get score\n",
    "    #if scoring == 'rmse':\n",
    "    rmse = mean_squared_error(test_targ, cv_pred, squared=False)\n",
    "    #else:\n",
    "    mae = mean_absolute_error(test_targ, cv_pred)\n",
    "    mape = mean_absolute_percentage_error(test_targ, cv_pred)\n",
    "    r2 = r2_score(test_targ, cv_pred)\n",
    "    rmse_scores.append(rmse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape)\n",
    "    r2_scores.append(r2)\n",
    "    preds.append(cv_pred)\n",
    "    # calculate number of folds after initial fold\n",
    "    num_folds = int((len(features)-(len(train_feats)+len(test_feats)))/fold_size)\n",
    "    # cv on rest of folds\n",
    "    for fold in range(num_folds):\n",
    "        train_feats, train_targ, test_feats, test_targ = features.iloc[0:init_fold+(fold_size*(fold+1)),:], target[0:init_fold+(fold_size*(fold+1))], features.iloc[init_fold+(fold_size*(fold+1)):init_fold+((fold_size*(fold+1))+fold_size),:], target[init_fold+(fold_size*(fold+1)):init_fold+((fold_size*(fold+1))+fold_size)]\n",
    "        model.fit(train_feats, train_targ)\n",
    "        cv_pred = model.predict(test_feats)\n",
    "        #if scoring == 'rmse':\n",
    "        rmse = mean_squared_error(test_targ, cv_pred, squared=False)\n",
    "        #else:\n",
    "        mae = mean_absolute_error(test_targ, cv_pred)\n",
    "        mape = mean_absolute_percentage_error(test_targ, cv_pred)\n",
    "        r2 = r2_score(test_targ, cv_pred)\n",
    "        rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        r2_scores.append(r2)\n",
    "        preds.append(cv_pred)\n",
    "    return rmse_scores, mae_scores, mape_scores, r2_scores, list(np.array(preds).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:06:45.755461Z",
     "start_time": "2021-09-23T08:06:45.723632Z"
    }
   },
   "outputs": [],
   "source": [
    "# weighted average functions\n",
    "def opt_ensemble_predictions(models, weights, features):\n",
    "    \"\"\"\n",
    "    make ensemble predictions, returns prediction\n",
    "    parameters:\n",
    "        models = list of models/pipes to include in ensemble\n",
    "        weights = array of weights to use for predictions\n",
    "        features = features used for predictions\n",
    "\n",
    "    returns:\n",
    "        predicted values\n",
    "    \"\"\"\n",
    "    # model predictions\n",
    "    yhats = [model.predict(features) for model in models]\n",
    "    yhats = np.array(yhats)\n",
    "    # weighted sum across models.. predictions of the weighted models\n",
    "    # summed products of predictions & weights\n",
    "    weighted_preds = np.tensordot(yhats, weights, axes=((0), (0)))\n",
    "    return weighted_preds\n",
    "\n",
    "def opt_ensemble_eval(models, weights, features, target):\n",
    "    \"\"\"evaluates optimal weighted ensemble by averaging rmse\"\"\"\n",
    "    yhats, _ = evaluate_ensemble(models, weights, features, target)\n",
    "    eval_targ = target[12:]\n",
    "    scores = []\n",
    "    for i in range(4):\n",
    "        rmse = mean_squared_error(eval_targ[(12*(i+1))-12:12*(i+1)], yhats[(12*(i+1))-12:12*(i+1)], squared=False)\n",
    "        scores.append(rmse)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def cv_pred(model, features, target):\n",
    "    \"\"\" time series cross validation \"\"\"\n",
    "    _, _, _, _, preds = expanding_window_cv(features, target, model, fold_size=12, init_fold=12)\n",
    "    return preds\n",
    "\n",
    "def ensemble_predictions(models, weights, features, target):\n",
    "    \"\"\"\n",
    "    make ensemble predictions, returns prediction\n",
    "    parameters:\n",
    "        models = list of models/pipes to include in ensemble\n",
    "        weights = array of weights to use for predictions\n",
    "        test_features = list of test features for each model included.. must be in same order\n",
    "\n",
    "    returns:\n",
    "        predicted classes, predicted probabilities for success, i.e label 1\n",
    "    \"\"\"\n",
    "    # model predictions\n",
    "    yhats = [cv_pred(models[i], features, target) for i in range(\n",
    "        len(models))]\n",
    "    yhats = np.array(yhats)\n",
    "    # weighted sum across models.. predictions of the weighted models\n",
    "    # summed products of predictions & weights\n",
    "    weighted_preds = np.tensordot(yhats, weights, axes=((0), (0)))\n",
    "    return weighted_preds\n",
    "\n",
    "\n",
    "def evaluate_ensemble(models, weights, features, target):\n",
    "    \"\"\"returns predicted classes, success probabilities and metric evaluation for models\"\"\"\n",
    "    # predictions\n",
    "    yhat = ensemble_predictions(models, weights, features, target)\n",
    "    # calculate fbeta loss\n",
    "    return yhat, mean_squared_error(target[12:], yhat, squared=False)\n",
    "\n",
    "\n",
    "def normalise(weights):\n",
    "    \"\"\"normalises weight vector to have unit norm, returns normalised weight vector\"\"\"\n",
    "    # l1 vector norm\n",
    "    result = np.linalg.norm(weights, 1)\n",
    "    # check for vector of all 0's\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    return weights / result\n",
    "\n",
    "\n",
    "def loss_function(weights, models, features, target):\n",
    "    \"\"\"loss function for optimisation, designed to be minimised\"\"\"\n",
    "    # normalise weights\n",
    "    norm_weights = normalise(weights)\n",
    "    # calculate error\n",
    "    _, score = evaluate_ensemble(\n",
    "        models, norm_weights, features, target)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T13:26:57.261622Z",
     "start_time": "2021-05-30T13:26:57.253685Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ml_models():\n",
    "    models, names = list(), list()\n",
    "    models.append(SVR())\n",
    "    names.append('SVR')\n",
    "    models.append(KNeighborsRegressor(n_jobs=-1))\n",
    "    names.append('KNN')\n",
    "    models.append(GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=3, max_features='auto', random_state=42))\n",
    "    names.append('GB')\n",
    "    models.append(AdaBoostRegressor(random_state=42))\n",
    "    names.append('AB')\n",
    "    models.append(ExtraTreesRegressor(n_estimators=100, max_depth=3, min_samples_split=3, max_features='auto', n_jobs=-1, random_state=42))\n",
    "    names.append('ET')\n",
    "    models.append(XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, min_child_weight=3, n_jobs=-1, random_state=42,))\n",
    "    names.append('XGB')\n",
    "    models.append(LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, min_child_samples=3, n_jobs=-1, random_state=42))\n",
    "    names.append('LGBM')\n",
    "    models.append(Earth())\n",
    "    names.append('MARS')\n",
    "    models.append(NGBRegressor(Dist=Exponential, n_estimators=100))\n",
    "    names.append('NGB')\n",
    "    models.append(LinearRegression(n_jobs=-1))\n",
    "    names.append('LR')\n",
    "    models.append(Ridge(random_state=42))\n",
    "    names.append('R')\n",
    "    models.append(SGDRegressor(shuffle=False, random_state=42))\n",
    "    names.append('SGD')\n",
    "    #models.append(LinearGAM(terms='s(0, n_splines=10)+s(1, n_splines=10)+s(2, n_splines=10)+s(3, n_splines=10)+l(4)'))\n",
    "    #names.append('LGAM')\n",
    "    return models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T11:39:25.743402Z",
     "start_time": "2021-09-14T11:39:25.725360Z"
    }
   },
   "outputs": [],
   "source": [
    "class KeepColumnsTransformer():\n",
    "    \"\"\" \n",
    "    custom transformer that keeps specified columns of dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        if len(self.columns) == 1:\n",
    "            drop_df = np.array(X[self.columns[0]].copy()).reshape(-1, 1)\n",
    "        elif type(X) == pd.Series:\n",
    "            drop_df = np.array(X[self.columns].copy()).reshape(1, -1)\n",
    "        else:\n",
    "            drop_df = X[self.columns].copy()\n",
    "        return drop_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Supervised Gas Consumption Modelling --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:00:50.121857Z",
     "start_time": "2021-05-13T11:00:41.332308Z"
    }
   },
   "outputs": [],
   "source": [
    "# load gas usage data\n",
    "gas_consumption = pd.read_csv('gas_consumption.csv')\n",
    "outside_temp = pd.read_csv('outside_temp.csv')\n",
    "# upstairs\n",
    "bedroom1_temp = pd.read_csv('bedroom1_temp.csv')\n",
    "bedroom2_temp = pd.read_csv('bedroom2_temp.csv')\n",
    "bedroom3_temp = pd.read_csv('bedroom3_temp.csv')\n",
    "bathroom_temp = pd.read_csv('bathroom_temp.csv')\n",
    "desk_temp = pd.read_csv('desk_temp.csv')\n",
    "# downstairs\n",
    "living_room_temp = pd.read_csv('living_room_temp.csv')\n",
    "kitchen_temp = pd.read_csv('kitchen_temp.csv')\n",
    "storage_temp = pd.read_csv('storage_temp.csv')\n",
    "# heating\n",
    "upstairs_heating = pd.read_csv('heating_upstairs.csv')\n",
    "downstairs_heating = pd.read_csv('heating_downstairs.csv')\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:01:38.510243Z",
     "start_time": "2021-05-13T11:01:38.004187Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove last months of 2013 and early months of 2020\n",
    "# split train/test\n",
    "# gas consumption\n",
    "gc_train = gas_consumption[gas_consumption['date_year'] < 2019]\n",
    "gc_test = gas_consumption[(gas_consumption['date_year'] > 2018) & (\n",
    "    gas_consumption['date_year'] < 2020)]\n",
    "# outside temperature\n",
    "ot_train = outside_temp[outside_temp['date_year'] < 2019]\n",
    "ot_test = outside_temp[(outside_temp['date_year'] > 2018)\n",
    "                       & (outside_temp['date_year'] < 2020)]\n",
    "# bedroom 1 temperature\n",
    "bedroom1_temp_train = bedroom1_temp[bedroom1_temp['date_year'] < 2019]\n",
    "bedroom1_temp_test = bedroom1_temp[(bedroom1_temp['date_year'] > 2018) & (\n",
    "    bedroom1_temp['date_year'] < 2020)]\n",
    "# bedroom 2 temperature\n",
    "bed2_train = bedroom2_temp[bedroom2_temp['date_year'] < 2019]\n",
    "bed2_test = bedroom2_temp[(bedroom2_temp['date_year'] > 2018) & (\n",
    "    bedroom2_temp['date_year'] < 2020)]\n",
    "# bedroom 3 temperature\n",
    "bed3_train = bedroom3_temp[bedroom3_temp['date_year'] < 2019]\n",
    "bed3_test = bedroom3_temp[(bedroom3_temp['date_year'] > 2018) & (\n",
    "    bedroom3_temp['date_year'] < 2020)]\n",
    "# bathroom temperature\n",
    "bath_train = bathroom_temp[bathroom_temp['date_year'] < 2019]\n",
    "bath_test = bathroom_temp[(bathroom_temp['date_year'] > 2018) & (\n",
    "    bathroom_temp['date_year'] < 2020)]\n",
    "# desk temperature\n",
    "desk_train = desk_temp[desk_temp['date_year'] < 2019]\n",
    "desk_test = desk_temp[(desk_temp['date_year'] > 2018) &\n",
    "                      (desk_temp['date_year'] < 2020)]\n",
    "# living room temperature\n",
    "lr_train = living_room_temp[living_room_temp['date_year'] < 2019]\n",
    "lr_test = living_room_temp[(living_room_temp['date_year'] > 2018) & (\n",
    "    living_room_temp['date_year'] < 2020)]\n",
    "# kitchen temperature\n",
    "k_train = kitchen_temp[kitchen_temp['date_year'] < 2019]\n",
    "k_test = kitchen_temp[(kitchen_temp['date_year'] > 2018)\n",
    "                      & (kitchen_temp['date_year'] < 2020)]\n",
    "# storage temperature\n",
    "st_train = storage_temp[storage_temp['date_year'] < 2019]\n",
    "st_test = storage_temp[(storage_temp['date_year'] > 2018)\n",
    "                       & (storage_temp['date_year'] < 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:01:43.662899Z",
     "start_time": "2021-05-13T11:01:43.498858Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "# monthly measurements\n",
    "# monthly avg temperature C\n",
    "ot_monthly_train = ot_train.groupby(by=['date_year', 'date_month']).agg({\n",
    "    'outside_temp_C': ['mean', 'median']})\n",
    "# monthly consumption of gas per year\n",
    "gc_monthly_train = gc_train.groupby(by=['date_year', 'date_month']).agg({\n",
    "    'gas_consumed_01_M3': ['min', 'max']})\n",
    "gc_monthly_train['gas_used'] = gc_monthly_train['gas_consumed_01_M3']['max'] - \\\n",
    "    gc_monthly_train['gas_consumed_01_M3']['min']\n",
    "gc_monthly_train['gas_used_M3'] = gc_monthly_train['gas_used'] * 0.1\n",
    "gc_monthly_train['avg_outside_tempC'] = np.round(\n",
    "    ot_monthly_train['outside_temp_C']['mean'], 2)\n",
    "# keep/remove columns\n",
    "gc_monthly_train = gc_monthly_train[['avg_outside_tempC', 'gas_used_M3']]\n",
    "# remove datetime index\n",
    "gc_monthly_train.reset_index(inplace=True)\n",
    "# season feature\n",
    "gc_monthly_train['season'] = np.nan\n",
    "# winter\n",
    "w_idx1 = gc_monthly_train[(gc_monthly_train['date_month'] >= 1) & (\n",
    "    gc_monthly_train['date_month'] <= 2)].index.tolist()\n",
    "w_idx2 = gc_monthly_train[gc_monthly_train['date_month'] == 12].index.tolist()\n",
    "winter_idx = w_idx1 + w_idx2\n",
    "gc_monthly_train.loc[winter_idx, 'season'] = 'winter'\n",
    "# spring\n",
    "spring_idx = gc_monthly_train[(gc_monthly_train['date_month'] >= 3) & (\n",
    "    gc_monthly_train['date_month'] <= 5)].index\n",
    "gc_monthly_train.loc[spring_idx, 'season'] = 'spring'\n",
    "# summer\n",
    "summer_idx = gc_monthly_train[(gc_monthly_train['date_month'] >= 6) & (\n",
    "    gc_monthly_train['date_month'] <= 8)].index\n",
    "gc_monthly_train.loc[summer_idx, 'season'] = 'summer'\n",
    "# autumn\n",
    "autumn_idx = gc_monthly_train[(gc_monthly_train['date_month'] >= 9) & (\n",
    "    gc_monthly_train['date_month'] <= 11)].index\n",
    "gc_monthly_train.loc[autumn_idx, 'season'] = 'autumn'\n",
    "# cyclical nature of month\n",
    "gc_monthly_train['month_sin'] = np.sin(2*np.pi*gc_monthly_train.date_month/12)\n",
    "gc_monthly_train['month_cos'] = np.cos(2*np.pi*gc_monthly_train.date_month/12)\n",
    "# cylical nature of season\n",
    "gc_monthly_train['season_label'] = 0\n",
    "w_idx = gc_monthly_train[gc_monthly_train['season'] == 'winter'].index\n",
    "gc_monthly_train.loc[w_idx, 'season_label'] = 1\n",
    "sp_idx = gc_monthly_train[gc_monthly_train['season'] == 'spring'].index\n",
    "gc_monthly_train.loc[sp_idx, 'season_label'] = 2\n",
    "su_idx = gc_monthly_train[gc_monthly_train['season'] == 'summer'].index\n",
    "gc_monthly_train.loc[su_idx, 'season_label'] = 3\n",
    "a_idx = gc_monthly_train[gc_monthly_train['season'] == 'autumn'].index\n",
    "gc_monthly_train.loc[a_idx, 'season_label'] = 4\n",
    "gc_monthly_train['season_sin'] = np.sin(\n",
    "    2*np.pi*gc_monthly_train.season_label/4)\n",
    "gc_monthly_train['season_cos'] = np.cos(\n",
    "    2*np.pi*gc_monthly_train.season_label/4)\n",
    "gc_monthly_train.drop(columns=['season_label'], level=0, inplace=True)\n",
    "gc_monthly_train = gc_monthly_train[\n",
    "    ['date_year', 'date_month', 'month_sin', 'month_cos', 'season', 'season_sin', 'season_cos', 'avg_outside_tempC', 'gas_used_M3']]\n",
    "gc_monthly_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:04:06.519338Z",
     "start_time": "2021-05-13T11:04:06.507337Z"
    }
   },
   "outputs": [],
   "source": [
    "# split & save train data\n",
    "gc_monthly_train_features = gc_monthly_train[[\n",
    "    'date_year', 'date_month', 'month_sin', 'month_cos', 'season', 'season_sin', 'season_cos', 'avg_outside_tempC']]\n",
    "gc_monthly_train_target = gc_monthly_train['gas_used_M3']\n",
    "save_data(gc_monthly_train_features,\n",
    "          'gas_consumption_monthly_train_features.csv')  # save features\n",
    "save_data(gc_monthly_train_target,\n",
    "          'gas_consumption_monthly_train_target.csv')  # save target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Season feature created as it is fair to say seasonality plays a big part in the use of gas for central heating, more gas used in the winter period compared to the summer period, the cyclical nature of season was extracted into a feature aswell as for month. Also from the section of data above we can see that outside temperature affects gas usage, more gas looks to be used when the temperature outside is colder, except for the first observation which could be due to lack of data for this month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:29:37.373475Z",
     "start_time": "2021-05-05T12:29:37.272440Z"
    }
   },
   "outputs": [],
   "source": [
    "# monthly avg temperature C\n",
    "ot_monthly_test = ot_test.groupby(by=['date_year', 'date_month']).agg({\n",
    "    'outside_temp_C': ['mean', 'median']})\n",
    "# monthly consumption of gas per year\n",
    "gc_monthly_test = gc_test.groupby(by=['date_year', 'date_month']).agg({\n",
    "    'gas_consumed_01_M3': ['min', 'max']})\n",
    "gc_monthly_test['gas_used'] = gc_monthly_test['gas_consumed_01_M3']['max'] - \\\n",
    "    gc_monthly_test['gas_consumed_01_M3']['min']\n",
    "gc_monthly_test['gas_used_M3'] = gc_monthly_test['gas_used'] * 0.1\n",
    "gc_monthly_test['avg_outside_tempC'] = np.round(\n",
    "    ot_monthly_test['outside_temp_C']['mean'], 2)\n",
    "# keep/remove columns\n",
    "gc_monthly_test = gc_monthly_test[['avg_outside_tempC', 'gas_used_M3']]\n",
    "# remove datetime index\n",
    "gc_monthly_test.reset_index(inplace=True)\n",
    "# season feature\n",
    "gc_monthly_train['season'] = np.nan\n",
    "# winter\n",
    "w_idx1 = gc_monthly_test[(gc_monthly_test['date_month'] >= 1) & (\n",
    "    gc_monthly_test['date_month'] <= 2)].index.tolist()\n",
    "w_idx2 = gc_monthly_test[gc_monthly_test['date_month'] == 12].index.tolist()\n",
    "winter_idx = w_idx1 + w_idx2\n",
    "gc_monthly_test.loc[winter_idx, 'season'] = 'winter'\n",
    "# spring\n",
    "spring_idx = gc_monthly_test[(gc_monthly_test['date_month'] >= 3) & (\n",
    "    gc_monthly_test['date_month'] <= 5)].index\n",
    "gc_monthly_test.loc[spring_idx, 'season'] = 'spring'\n",
    "# summer\n",
    "summer_idx = gc_monthly_test[(gc_monthly_test['date_month'] >= 6) & (\n",
    "    gc_monthly_test['date_month'] <= 8)].index\n",
    "gc_monthly_test.loc[summer_idx, 'season'] = 'summer'\n",
    "# autumn\n",
    "autumn_idx = gc_monthly_test[(gc_monthly_test['date_month'] >= 9) & (\n",
    "    gc_monthly_test['date_month'] <= 11)].index\n",
    "gc_monthly_test.loc[autumn_idx, 'season'] = 'autumn'\n",
    "# cyclical nature of month\n",
    "gc_monthly_test['month_sin'] = np.sin(2*np.pi*gc_monthly_test.date_month/12)\n",
    "gc_monthly_test['month_cos'] = np.cos(2*np.pi*gc_monthly_test.date_month/12)\n",
    "# cylical nature of season\n",
    "gc_monthly_test['season_label'] = 0\n",
    "w_idx = gc_monthly_test[gc_monthly_test['season'] == 'winter'].index\n",
    "gc_monthly_test.loc[w_idx, 'season_label'] = 1\n",
    "sp_idx = gc_monthly_test[gc_monthly_test['season'] == 'spring'].index\n",
    "gc_monthly_test.loc[sp_idx, 'season_label'] = 2\n",
    "su_idx = gc_monthly_test[gc_monthly_test['season'] == 'summer'].index\n",
    "gc_monthly_test.loc[su_idx, 'season_label'] = 3\n",
    "a_idx = gc_monthly_test[gc_monthly_test['season'] == 'autumn'].index\n",
    "gc_monthly_test.loc[a_idx, 'season_label'] = 4\n",
    "gc_monthly_test['season_sin'] = np.sin(2*np.pi*gc_monthly_test.season_label/4)\n",
    "gc_monthly_test['season_cos'] = np.cos(2*np.pi*gc_monthly_test.season_label/4)\n",
    "gc_monthly_test.drop(columns=['season_label'], level=0, inplace=True)\n",
    "gc_monthly_test = gc_monthly_test[\n",
    "    ['date_year', 'date_month', 'month_sin', 'month_cos', 'season', 'season_sin', 'season_cos', 'avg_outside_tempC', 'gas_used_M3']]\n",
    "gc_monthly_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:29:43.934934Z",
     "start_time": "2021-05-05T12:29:43.917960Z"
    }
   },
   "outputs": [],
   "source": [
    "# split & save test data\n",
    "gc_monthly_test_features = gc_monthly_test[[\n",
    "    'date_year', 'date_month', 'month_sin', 'month_cos', 'season', 'season_sin', 'season_cos', 'avg_outside_tempC']]\n",
    "gc_monthly_test_target = gc_monthly_test['gas_used_M3']\n",
    "save_data(gc_monthly_test_features,\n",
    "          'gas_consumption_monthly_test_features.csv')  # save features\n",
    "save_data(gc_monthly_test_target,\n",
    "          'gas_consumption_monthly_test_target.csv')  # save target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data Cleansing -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T12:57:43.173489Z",
     "start_time": "2021-05-10T12:57:43.134309Z"
    }
   },
   "outputs": [],
   "source": [
    "mt_features, mt_target = load_train_data()\n",
    "mt_features['gas_used_M3'] = mt_target\n",
    "# analyse without lag nan values\n",
    "monthly_train_features = mt_features[(mt_features['date_year']>=2015)&(mt_features['date_year']<=2018)]\n",
    "monthly_train_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 62 observations and no nan values are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T12:59:38.870568Z",
     "start_time": "2021-05-10T12:59:38.721817Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outliers\n",
    "mt_features, mt_target = load_train_data()\n",
    "mt_features['gas_used_M3'] = mt_target\n",
    "# analyse without lag nan values\n",
    "monthly_train_features = mt_features[(mt_features['date_year']>=2015)&(mt_features['date_year']<=2018)]\n",
    "for col in monthly_train_features:\n",
    "    # ony want to check outliers for temperature and gas used\n",
    "    if monthly_train_features[col].dtype == 'float64' or monthly_train_features[col].dtype == 'int64':\n",
    "        stats = monthly_train_features[col].describe()\n",
    "        IQR = stats['75%'] - stats['25%']\n",
    "        upper = stats['75%'] + 1.5 * IQR\n",
    "        lower = stats['25%'] - 1.5 * IQR\n",
    "        lower_bound_outliers = monthly_train_features[monthly_train_features[col] < lower]\n",
    "        upper_bound_outliers = monthly_train_features[monthly_train_features[col] > upper]\n",
    "\n",
    "        print('{}: {} upper outliers and {} lower outliers, bounds: upper bound = {}, lower bound = {}\\n'.format(\n",
    "            col, len(upper_bound_outliers), len(lower_bound_outliers), upper, lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the interquartile range, no outliers are present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T11:13:34.444783Z",
     "start_time": "2021-05-05T11:13:34.389375Z"
    }
   },
   "outputs": [],
   "source": [
    "gc_monthly_train.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No anomolous data can be seen so far, avg_outside_tempC looks as if it is fairly normally distributed with the mean and median values close together. gas_used_M3 seems to have a right skewed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - EDA -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T05:13:48.135858Z",
     "start_time": "2021-06-08T05:13:48.061456Z"
    }
   },
   "outputs": [],
   "source": [
    "monthly_train_features,_ = load_train_data()\n",
    "monthly_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T11:04:02.713169Z",
     "start_time": "2021-08-20T11:04:02.698135Z"
    }
   },
   "outputs": [],
   "source": [
    "# categorise features by data type\n",
    "measured_cont = [\n",
    "    'avg_outside_tempC', 'gas_used_M3', 'lag_12', 'lag_1', 'roll_mean_targ12', 'roll_mean_targ1', 'avg_tempC_lag12',\n",
    "    'avg_tempC_lag1', 'tempC_exp_mean_per_month','gas_used_exp_mean_per_month','hodnhr_exp_mean_per_month',\n",
    "    'avg_downstairs_tempC','avg_upstairs_tempC','hounhr_exp_mean_per_month','avg_dwn_temp_exp_mean_per_month',\n",
    "    'avg_up_temp_exp_mean_per_month','avg_downstairs_temp_lag12','avg_downstairs_temp_lag1','avg_upstairs_temp_lag12',\n",
    "    'avg_upstairs_temp_lag1','avg_house_tempC','avg_house_tempC_lag12','avg_house_tempC_lag1','exp_mean_ratio_outsidetemp_gas_used',\n",
    "    'avg_house_temp_exp_mean_per_month','exp_mean_ratio_housetemp_gas_used','exp_mean_ratio_hodnhr_gas_used',\n",
    "    'exp_mean_ratio_hounhr_gas_used','avg_tempC_lag12_power4','tempC_exp_mean_per_month_power4',\n",
    "    'exp_mean_ratio_outsidetemp_gas_used_power4','exp_mean_ratio_housetemp_gas_used_power4','knn_local_knowledge',\n",
    "    'roll_mean_targ1*mnth_sin','roll_mean_targ1*mnth_cos','exp_mean_ratio_outsidetemp_gas_used*mnth_sin',\n",
    "    'exp_mean_ratio_outsidetemp_gas_used*mnth_cos','exp_mean_ratio_housetemp_gas_used*mnth_sin',\n",
    "    'exp_mean_ratio_housetemp_gas_used*mnth_cos','knn_local_knowledge*mnth_sin','knn_local_knowledge*mnth_cos',\n",
    "    'gas_used_exp_mean_per_season','roll_mean_targ1_lag_12']\n",
    "discrete_cont = ['date_year', 'days_in_month','heat_on_dwn_num_hourly_recordings','heat_on_up_num_hourly_recordings',\n",
    "                'hodnhr_lag12','hodnhr_lag1','hounhr_lag12','hounhr_lag1']\n",
    "nominal = ['season', 'date_month', 'quarter']\n",
    "cyclical = ['month_sin', 'month_cos', 'season_sin', 'season_cos', 'quarter_sin', 'quarter_cos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution/Relationship Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T11:21:54.271599Z",
     "start_time": "2021-05-16T11:21:41.690640Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature distributions and target relationships\n",
    "mt_features, mt_target = load_train_data()\n",
    "mt_features['gas_used_M3'] = mt_target\n",
    "# analyse without lag nan values\n",
    "monthly_train_features = mt_features[(mt_features['date_year']>=2015)&(mt_features['date_year']<=2018)]\n",
    "\n",
    "for col in monthly_train_features:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    if col != 'gas_used_M3':\n",
    "        if col not in nominal:\n",
    "            if col in discrete_cont:\n",
    "                sns.histplot(data=monthly_train_features, x=col,\n",
    "                             stat='count', discrete=True, kde=True, ax=ax1)\n",
    "            elif col in measured_cont:\n",
    "                sns.histplot(data=monthly_train_features, x=col,\n",
    "                             stat='count', bins=20, kde=True, ax=ax1)\n",
    "            else:\n",
    "                sns.histplot(data=monthly_train_features, x=col,\n",
    "                             stat='count', bins=5, kde=True, ax=ax1)\n",
    "            ax1.set_title(f'{col} Distribution')\n",
    "            ax1.set_xlabel(f'{col}')\n",
    "            ax1.set_ylabel('Count')\n",
    "            ax1.grid(alpha=.3)\n",
    "\n",
    "            sns.scatterplot(x=col, y='gas_used_M3',\n",
    "                            data=monthly_train_features, ax=ax2)\n",
    "            ax2.set_title(f'{col}/gas_used_M3 relationship')\n",
    "            ax2.set_xlabel(f'{col}')\n",
    "            ax2.set_ylabel('Gas Used (M3)')\n",
    "        else:\n",
    "            if col == 'season': # apply ordering\n",
    "                monthly_train_features['season'] = monthly_train_features['season'].astype(\n",
    "                    'category')\n",
    "                monthly_train_features['season'] = monthly_train_features['season'].cat.reorder_categories(\n",
    "                    ['winter', 'spring', 'summer', 'autumn'], ordered=True)\n",
    "\n",
    "            sns.barplot(x=monthly_train_features[col].value_counts(\n",
    "            ).index, y=monthly_train_features[col].value_counts().values, ax=ax1)\n",
    "            ax1.set_title(f'{col} Distribution')\n",
    "            ax1.set_xlabel(f'{col}')\n",
    "            ax1.set_ylabel('Frequency')\n",
    "            ax1.grid(alpha=.3)\n",
    "\n",
    "            sns.boxplot(x=col, y='gas_used_M3',\n",
    "                        data=monthly_train_features, ax=ax2)\n",
    "            ax2.set_title(f'{col}/gas_used_M3 relationship')\n",
    "            ax2.set_xlabel(f'{col}')\n",
    "            ax2.set_ylabel('Gas Used (M3)')\n",
    "    else:\n",
    "        continue\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of the 2 months at the end of 2013, date_year and date_month have a uniform distribtution which is to be expected. date_month has a non linear relationship with gas usage, it shows seasonality where more gas is used in the winter months and hardly any used in the summer months. season has a uniform distribution with the exception of the 2 months which is as expected, it also confirms seasonality with gas usage, with more being used in the winter months and less being used in the summer months, spring and autumn show similar usage. avg_outside_tempC appears to be bimodal with a peak approx. around 6 dergrees C, with some digging the values around this peak are of winter months, the values around the other peak at approx. 18 have a seasonal mode of summer. There also appears to be a strong negative non linear relationship between gas usage and outside temperature which is to be expected. As the cyclical data is extracted from features with apparent relationships with gas usage they also will have relationships.\n",
    "\n",
    "The relationship between quartely and gas_used_M3 also highlights that more gas is used in the colder months than the warmer months. We can see that more gas is used if a month has 31 days but this will be predominantly of the months december and january, it also highlights the gas used for february. Using the previous years values (lag 12) for gas_used_M3 we can see a fairly strong positive linear relationship compared to the previous month (lag 1) where there is not as strong a relationship.\n",
    "We can see  that a rolling mean with a window of 12 previous months gas usage doesnt show any relationship with gas usage, but a rolling mean with a window of 2 previous months gas usage shows a strong positive linear relationship with gas usage. The previous years average temperature (avg_tempC_lag12) shows a fairly strong slight non linear negative relationship with gas usage compared to the previous months temperature which still shows a negative relationship but appears to be not as strong. The expanding mean for avg temperature for each month shows a strong non-linear negative relationship with gas usage with what appears to be a bimodal distribution, again this would be due to the temperature for colder and warmer months. The expanding mean for gas usage also shows a strong positive maybe slightly non linear relationship with gas usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T13:12:35.168059Z",
     "start_time": "2021-05-10T13:12:34.871143Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target distribution\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['gas_used_M3'] = monthly_train_target\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(x='gas_used_M3', data=monthly_train_features, bins=10, stat='count', kde=True)\n",
    "plt.title('Gas Used Distribution')\n",
    "plt.xlabel('Gas Used (M3)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target distribution shows a high number of observations approx. below 26 units, these observations are made up of mainly summer months with a few autumn and spring months, mainly may and september which are either side of the summer period and can be expected usage amounts for these months. Nothing appears unexpected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T11:11:56.316216Z",
     "start_time": "2021-08-20T11:11:55.891198Z"
    }
   },
   "outputs": [],
   "source": [
    "# normality statistical test at 95% confidence\n",
    "# H0 = feature comes from a normal distribution\n",
    "# Ha = feature does not come from a normal distribution\n",
    "# alpha = 5%\n",
    "mt_features, _ = load_train_data()\n",
    "gaussian = []\n",
    "non_gaussian = []\n",
    "\n",
    "for col in mt_features.columns:\n",
    "    alpha = 0.05\n",
    "    if col in measured_cont:\n",
    "        print(f'\\n{col}:')\n",
    "        feat = mt_features[col].copy()\n",
    "        shap_stat, shap_p = shapiro(feat)\n",
    "        dagos_stat, dagos_p = normaltest(feat)\n",
    "        stat_result = anderson(feat)\n",
    "\n",
    "        count = 0\n",
    "        if shap_p > alpha:\n",
    "            count += 1\n",
    "            print(\n",
    "                f'Shapiro test:  Sample looks gaussian, stat = {shap_stat}, p = {shap_p}')\n",
    "        else:\n",
    "            print(\n",
    "                f'Shapiro test:  Sample does not look gaussian, stat = {shap_stat}, p = {shap_p}')\n",
    "\n",
    "        if dagos_p > alpha:\n",
    "            count += 1\n",
    "            print(\n",
    "                f\"D'agostino test:  Sample looks gaussian, stat = {dagos_stat}, p = {dagos_p}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"D'agostino test:  Sample does not look gaussian, stat = {dagos_stat}, p = {dagos_p}\")\n",
    "\n",
    "        if stat_result.statistic < stat_result.critical_values[0]:\n",
    "            count += 1\n",
    "            print(f\"Anderson test:  Sample looks gaussian\")\n",
    "        else:\n",
    "            print(f\"Anderson test:  Sample does not look gaussian\")\n",
    "        for i in range(len(stat_result.critical_values)):\n",
    "            sl, cv = stat_result.significance_level[i], stat_result.critical_values[i]\n",
    "            print(\n",
    "                f\"Anderson test:  stat = {stat_result.statistic}, sig = {sl}, crit val = {cv}\")\n",
    "\n",
    "        if count >= 2:\n",
    "            gaussian.append(col)\n",
    "        elif count <= 1:\n",
    "            non_gaussian.append(col)\n",
    "\n",
    "print(f'\\nGaussian Features: {gaussian}')\n",
    "print(f'\\nNon Gaussian Features: {non_gaussian}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T16:08:14.605852Z",
     "start_time": "2021-05-05T16:08:14.135916Z"
    }
   },
   "outputs": [],
   "source": [
    "# seasonal gas usage distributions\n",
    "# remove last 2 months of 2013\n",
    "mtf = monthly_train_features[(monthly_train_features['date_year'] >= 2014) & (\n",
    "    monthly_train_features['date_year'] <= 2018)]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plt.suptitle('Seasonal Distribution of Gas Usage')\n",
    "sns.kdeplot(data=mtf, x='avg_outside_tempC',\n",
    "            y='gas_used_M3', hue='season', ax=ax1)\n",
    "ax1.grid(alpha=.2)\n",
    "sns.histplot(data=mtf, x='gas_used_M3', hue='season', element='poly', ax=ax2)\n",
    "ax2.grid(alpha=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From plots above we can see the extent of gas usage for seasonality, winter occupies the higher values of gas usage, summer densely occupies the lower values of gas usage whilst there is a similar spread of usage for spring and autumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T16:31:18.352179Z",
     "start_time": "2021-05-05T16:31:18.119488Z"
    }
   },
   "outputs": [],
   "source": [
    "# monthly gas usage distributions\n",
    "# remove last 2 months of 2013\n",
    "mtf = monthly_train_features[(monthly_train_features['date_year'] >= 2014) & (\n",
    "    monthly_train_features['date_year'] <= 2018)]\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.suptitle('Monthly Distribution of Gas Usage')\n",
    "sns.scatterplot(data=mtf, x='date_month', y='gas_used_M3', hue='avg_outside_tempC',\n",
    "                size='avg_outside_tempC', sizes=(20, 200), hue_norm=(0, 7), legend='brief', cmap='blue')\n",
    "plt.grid(alpha=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, again we can say that outside temperature and month has a direct affect on household gas usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:46:44.481684Z",
     "start_time": "2021-05-06T10:46:43.993875Z"
    }
   },
   "outputs": [],
   "source": [
    "# proportion of gas usage per season and month per year\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['gas_used_M3'] = monthly_train_target\n",
    "\n",
    "mtf = monthly_train_features[(monthly_train_features['date_year'] >= 2014) & (\n",
    "    monthly_train_features['date_year'] <= 2018)]\n",
    "# proportion of gas per season per year plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "plt.tight_layout(pad=4)\n",
    "for yr in range(2014, 2019):\n",
    "    yr_df = mtf[mtf['date_year'] == yr]\n",
    "    yr_gas_total = np.round(yr_df['gas_used_M3'].sum(), 2)\n",
    "    yr_props = []\n",
    "    season = []\n",
    "    for s in ['winter', 'spring', 'summer', 'autumn']:\n",
    "        season.append(s)\n",
    "        seas_gas_total = np.round(\n",
    "            yr_df[yr_df['season'] == s]['gas_used_M3'].sum(), 2)\n",
    "        gas_prop = np.round(seas_gas_total / yr_gas_total, 2)\n",
    "        yr_props.append(gas_prop)\n",
    "    sns.lineplot(x=season, y=yr_props, markers=True, label=yr, ax=ax1)\n",
    "ax1.set_title('Propotion of Gas Used Per Season')\n",
    "ax1.set_xlabel('Season')\n",
    "ax1.set_ylabel('Proportion of Gas Used')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(alpha=.2)\n",
    "# proportion of gas per month per year\n",
    "for yr in range(2014, 2019):\n",
    "    yr_df = mtf[mtf['date_year'] == yr]\n",
    "    yr_gas_total = np.round(yr_df['gas_used_M3'].sum(), 2)\n",
    "    yr_props = []\n",
    "    month = []\n",
    "    for mnth in range(1, 13):\n",
    "        month.append(mnth)\n",
    "        month_gas_total = np.round(\n",
    "            float(yr_df[yr_df['date_month'] == mnth]['gas_used_M3'].values), 2)\n",
    "        gas_prop = np.round(month_gas_total / yr_gas_total, 2)\n",
    "        yr_props.append(gas_prop)\n",
    "    sns.lineplot(x=month, y=yr_props, markers=True, label=yr, ax=ax2)\n",
    "ax2.set_title('Propotion of Gas Used Per Month')\n",
    "ax2.set_xlabel('Month')\n",
    "ax2.set_ylabel('Proportion of Gas Used')\n",
    "ax2.legend(loc='best')\n",
    "ax2.grid(alpha=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:51:29.620174Z",
     "start_time": "2021-05-06T10:51:29.071093Z"
    }
   },
   "source": [
    "As can be seen above the proportion of gas usage is fairly consistent across seasons, as for monthly proportions there seems to be some variability between february and march. This appears to be due to colder temperatures, 2017: feb-march = 5.78-9.32 degrees, 2018: feb-march = 0.08-5.49 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T13:15:11.757919Z",
     "start_time": "2021-05-07T13:15:11.194470Z"
    }
   },
   "outputs": [],
   "source": [
    "# month temperature for year\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "\n",
    "mtf = monthly_train_features[(monthly_train_features['date_year'] >= 2014) & (\n",
    "    monthly_train_features['date_year'] <= 2018)]\n",
    "\n",
    "\n",
    "years = sorted(mtf['date_year'].unique())\n",
    "# proportion of gas per season per year plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "for yr in years:\n",
    "    yr_df = mtf[mtf['date_year'] == yr]\n",
    "    mnth_temps = []\n",
    "    months = sorted(mtf['date_month'].unique())\n",
    "    for mnth in months:\n",
    "        mnth_temp = np.round(\n",
    "            float(yr_df[yr_df['date_month'] == mnth]['avg_outside_tempC'].values), 2)\n",
    "        mnth_temps.append(mnth_temp)\n",
    "    sns.lineplot(x=months, y=mnth_temps, markers=True, label=yr)\n",
    "plt.title('Monthly Temperature C per Year')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Temperature')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T11:09:28.788877Z",
     "start_time": "2021-05-06T11:09:28.769463Z"
    }
   },
   "source": [
    "### Correlational Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T05:26:33.773574Z",
     "start_time": "2021-05-27T05:26:19.230545Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# heatmap for colinearity\n",
    "mt_features, mt_target = load_train_data()\n",
    "mt_features['gas_used_M3'] = mt_target\n",
    "# analyse without lag nan values\n",
    "monthly_train_features = mt_features[(mt_features['date_year']>=2015)&(mt_features['date_year']<=2018)]\n",
    "\n",
    "heatmap_dict = {}\n",
    "columns = []\n",
    "\n",
    "for col1 in monthly_train_features:\n",
    "    if col1 not in cyclical:\n",
    "        columns.append(col1)\n",
    "        corr_vals = []\n",
    "        xf = monthly_train_features[col1]\n",
    "        for col2 in monthly_train_features:\n",
    "            if col2 not in cyclical:\n",
    "                yf = monthly_train_features[col2]\n",
    "                if col1 in measured_cont and col2 in measured_cont:\n",
    "                    score = spearmanr(xf, yf)[0]\n",
    "                    corr_vals.append(score)\n",
    "                if col1 in measured_cont and col2 in discrete_cont:\n",
    "                    score = spearmanr(xf, yf)[0]\n",
    "                    corr_vals.append(score)\n",
    "                if col1 in measured_cont and col2 in nominal:\n",
    "                    score = correlation_ratio(yf, xf)\n",
    "                    corr_vals.append(score)\n",
    "                if col1 in discrete_cont and col2 in discrete_cont:\n",
    "                    score = spearmanr(xf, yf)[0]\n",
    "                    corr_vals.append(score)\n",
    "                if col1 in discrete_cont and col2 in measured_cont:\n",
    "                    score = spearmanr(xf, yf)[0]\n",
    "                    corr_vals.append(score)\n",
    "                if col1 in discrete_cont and col2 in nominal:\n",
    "                    score = correlation_ratio(yf, xf)\n",
    "                    corr_vals.append(score)\n",
    "                if col1 in nominal and col2 in nominal:\n",
    "                    score = theils_u(xf, yf)\n",
    "                    corr_vals.append(score)\n",
    "                if col1 in nominal and col2 in measured_cont:\n",
    "                    score = correlation_ratio(xf, yf)\n",
    "                    corr_vals.append(score)\n",
    "                if col1 in nominal and col2 in discrete_cont:\n",
    "                    score = correlation_ratio(xf, yf)\n",
    "                    corr_vals.append(score)\n",
    "        heatmap_dict[col1] = corr_vals\n",
    "corr_grid = pd.DataFrame(heatmap_dict, index=columns)\n",
    "plt.figure(figsize=(30, 28))\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "sns.heatmap(corr_grid, vmin=-1, vmax=1, annot=True,\n",
    "            cmap='RdBu_r', linewidths=0.3)\n",
    "plt.yticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Gaussian or gaussian like distributions are not present among measured and discrete features, the relationships between measured features, discrete features and combination of both features will be tested using spearmans rho coefficient as it does not have distributional assumptions. For relationships involving measured/discrete features and nominal features the correlation ratio will be used as it uses the levels of the nominal feature to determine relationship strength. For nominal/nominal relationships theils u coefficient will be used.\n",
    "\n",
    "From the heatmap we can see that date_month, season & quarter show high association with gas usage, the correlation ratio says that given a gas usage measurement we are able to know which month, season or quarter it belongs to. We can see that date_year, days_in_month & roll_mean_lag12 have no correlation with gas usage and all other features have high to very high correlation with gas usage. It is clear we have high multicolinearity which is not a suprise as all features are derived from other features. With 95% confidence we can say that all relationships except date_year, days_in_month and roll_mean_lag12 with the target gas_used_M3 are all statistically significant. From this analysis we can drop date_year, days_in_month, roll_mean_lag12, gas usage with lag_1 as lag_12 is better correlated, avg_tempC_lag12 & lag1 as tempC_exp_mean_per_month is better correlated, also avg_outside_tempC as we will have to forecast this and the expanding mean will likely do the same job. As date_month shows the highest association with our target we could opt to to solely use this feature as to avoid multicolinearity but I feel dimensionality reduction via PCA could be a better option as interpretability is not vital. We can assess this further when we come to model feature selection.\n",
    "\n",
    "***** After Feature Engineering *****\n",
    "The top correlated/associated features that will be kept are; date_month (sin & cosine transformed version), exp_mean_ratio_outsidetemp_gas_used (the ratio of outside temperature and gas used expanding means), exp_mean_ratio_housetemp_gas_used (the ratio of house temperature and gas used expanding means) and knn_local_knowledge (knn model of 5 nearest neighbours). Although roll_mean_targ1 shows high correlation this feature will not be useful to use as rolling means are for short term forecasts (1 step ahead (in our case 1 month ahead)) and we are forecasting 12 steps (12 months) ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T12:15:45.229171Z",
     "start_time": "2021-05-06T12:15:45.220131Z"
    }
   },
   "source": [
    "### Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:32:22.008682Z",
     "start_time": "2021-05-13T11:32:21.949674Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# statistical significance tests between features and target\n",
    "mt_features, mt_target = load_train_data()\n",
    "mt_features['gas_used_M3'] = mt_target\n",
    "# analyse without lag nan values\n",
    "monthly_train_features = mt_features[(mt_features['date_year']>=2015)&(mt_features['date_year']<=2018)]\n",
    "sig = []\n",
    "non_sig = []\n",
    "alpha = 0.05\n",
    "print('With 95% confidence:\\n')\n",
    "for col in monthly_train_features:\n",
    "    feat = monthly_train_features[col]\n",
    "    target = monthly_train_features['gas_used_M3']\n",
    "    if col in measured_cont:\n",
    "        if col == 'gas_used_M3':\n",
    "            continue\n",
    "        else:\n",
    "            spear_stat = spearmanr(feat, target.squeeze())\n",
    "            if spear_stat[1] > alpha:\n",
    "                print(\n",
    "                    f'{col} is not statistically significant, relationship likely to have occured by chance\\nSpearman correlation: {spear_stat[0]}, Pvalue : {spear_stat[1]}\\n')\n",
    "                non_sig.append(col)\n",
    "            else:\n",
    "                print(\n",
    "                    f'{col} is statistically significant, relationship not likely to have occured by chance\\nSpearman correlation: {spear_stat[0]}, Pvalue : {spear_stat[1]}\\n')\n",
    "                sig.append(col)\n",
    "    if col in discrete_cont:\n",
    "        kruskal_stat = kruskal(feat, target.squeeze())\n",
    "        if kruskal_stat[1] > alpha:\n",
    "            print(\n",
    "                f'{col} is not statistically significant, relationship likely to have occured by chance\\nKruskal statistic: {kruskal_stat[0]}, Pvalue : {kruskal_stat[1]}\\n')\n",
    "            non_sig.append(col)\n",
    "        else:\n",
    "            print(\n",
    "                f'{col} is statistically significant, relationship not likely to have occured by chance\\nKruskal statistic: {kruskal_stat[0]}, Pvalue : {kruskal_stat[1]}\\n')\n",
    "            sig.append(col)\n",
    "    if col in nominal:\n",
    "        le = LabelEncoder()\n",
    "        feat_enc = le.fit_transform(feat)\n",
    "        kruskal_stat = kruskal(feat_enc, target.squeeze())\n",
    "        if kruskal_stat[1] > alpha:\n",
    "            print(\n",
    "                f'{col} is not statistically significant, relationship likely to have occured by chance\\nKruskal statistic: {kruskal_stat[0]}, Pvalue : {kruskal_stat[1]}\\n')\n",
    "            non_sig.append(col)\n",
    "        else:\n",
    "            print(\n",
    "                f'{col} is statistically significant, relationship not likely to have occured by chance\\nKruskal statistic: {kruskal_stat[0]}, Pvalue : {kruskal_stat[1]}\\n')\n",
    "            sig.append(col)\n",
    "\n",
    "print(f'\\nSignificant:\\n{sig}\\n\\nNon-significant:\\n{non_sig}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statistical siginificance hypothesis tests with significance level of 5% (confidence level of 95%), Ho = feature/target relationship likely to have occured by chance, Ha = feature/target relationship not likely to have occured bu chance.\n",
    "Measured features tested using spearmans rho test, discrete features tested using kruskall-wallis test and nominal features also tested with kruskall-wallis test.\n",
    "\n",
    "The majority of features, original and engineered, appear to have statistically significant relationships with the target gas used, significance suggests that the relationship between feature and target (if there is one) is not likely to have occured by chance, non-significance suggests the relationship (if there is one) is likely to have occured by chance. Features with non-significant relationships will be dropped as their relationships are likely to have occured by chance and therefore may effect generalisation on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T12:05:01.851533Z",
     "start_time": "2021-05-13T12:04:59.301493Z"
    }
   },
   "outputs": [],
   "source": [
    "# normality plots and transforms for best features\n",
    "monthly_train_features,_ = load_clean_train_data()\n",
    "yj_pt = PowerTransformer(method='yeo-johnson')\n",
    "bc_pt = PowerTransformer(method='box-cox')\n",
    "qt = QuantileTransformer(\n",
    "    n_quantiles=len(monthly_train_features), output_distribution='normal', random_state=1)\n",
    "\n",
    "for col in monthly_train_features:\n",
    "    fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(10,10))\n",
    "    plt.suptitle(f'{col} normality qqplots')\n",
    "    plt.tight_layout(pad=3)\n",
    "    # original distribution\n",
    "    qqplot(monthly_train_features[col], line='s', ax=ax1)\n",
    "    ax1.set_title(f'{col} original')\n",
    "    if monthly_train_features[col].min() <= 0:\n",
    "        # yeo-johnson transform\n",
    "        yj_trans = yj_pt.fit_transform(np.array(monthly_train_features[col]).reshape(-1,1))\n",
    "        qqplot(yj_trans, line='s', ax=ax2)\n",
    "        ax2.set_title(f'{col} yeo-johnson transform')\n",
    "    else:\n",
    "        # yeo-johnson transform\n",
    "        yj_trans = yj_pt.fit_transform(np.array(monthly_train_features[col]).reshape(-1,1))\n",
    "        qqplot(yj_trans, line='s', ax=ax2)\n",
    "        ax2.set_title(f'{col} yeo-johnson transform')\n",
    "        # boxcox transform\n",
    "        bxcx_trans = bc_pt.fit_transform(np.array(monthly_train_features[col]).reshape(-1,1))\n",
    "        qqplot(bxcx_trans, line='s', ax=ax3)\n",
    "        ax3.set_title(f'{col} boxcox transform')\n",
    "    # quantile transform\n",
    "    quant_trans = qt.fit_transform(np.array(monthly_train_features[col]).reshape(-1,1))\n",
    "    qqplot(quant_trans, line='s', ax=ax4)\n",
    "    ax4.set_title(f'{col} quantile transform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the original features are non normally distributed and power transforms are unable to make them more gaussian like, we are not going to be able to infer results from linear models but as our focus is on forecasting future gas usage and not inference this will not effect us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial/Auto Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T12:19:07.066045Z",
     "start_time": "2021-05-07T12:19:06.722091Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, monthly_train_target = load_train_data()\n",
    "plot_acf(monthly_train_target, lags=12)\n",
    "plot_pacf(monthly_train_target, lags=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T12:14:10.066015Z",
     "start_time": "2021-08-20T12:14:09.848134Z"
    }
   },
   "outputs": [],
   "source": [
    "# pca on top features\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "# remove 12 month lag nan values\n",
    "monthly_train_features = monthly_train_features.iloc[12:,:]\n",
    "sc = StandardScaler()\n",
    "pca_df = sc.fit_transform(monthly_train_features)\n",
    "# fit pca algorithm\n",
    "pca = PCA(whiten=True, random_state=42).fit(pca_df)\n",
    "# Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)')  # for each component\n",
    "plt.title('Explained Variance of Features')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retain appox. 99% variance by reducing the data to 2 components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Feature Engineering -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:10:08.286369Z",
     "start_time": "2021-05-13T11:10:08.260369Z"
    }
   },
   "outputs": [],
   "source": [
    "# create quarter and num days in month\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "str_yr = monthly_train_features['date_year'].astype(int).astype(str)\n",
    "str_mnth = monthly_train_features['date_month'].astype(int).astype(str)\n",
    "str_date = str_yr.str.cat(str_mnth, sep='-')\n",
    "str_date = pd.to_datetime(str_date)\n",
    "monthly_train_features['quarter'] = str_date.dt.quarter\n",
    "monthly_train_features['days_in_month'] = str_date.dt.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:10:19.513586Z",
     "start_time": "2021-05-13T11:10:19.489586Z"
    }
   },
   "outputs": [],
   "source": [
    "# target lag feature\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['lag_12'] = monthly_train_target.shift(12)\n",
    "monthly_train_features['lag_1'] = monthly_train_target.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:10:28.886001Z",
     "start_time": "2021-05-13T11:10:28.863009Z"
    }
   },
   "outputs": [],
   "source": [
    "# target rolling mean\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['roll_mean_targ12'] = monthly_train_target.rolling(window=12).mean()\n",
    "monthly_train_features['roll_mean_targ1'] = monthly_train_target.rolling(window=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:10:40.830196Z",
     "start_time": "2021-05-13T11:10:40.801185Z"
    }
   },
   "outputs": [],
   "source": [
    "# temperature lag feature\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['avg_tempC_lag12'] = monthly_train_features['avg_outside_tempC'].shift(12)\n",
    "monthly_train_features['avg_tempC_lag1'] = monthly_train_features['avg_outside_tempC'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:11:15.444771Z",
     "start_time": "2021-05-13T11:11:15.367758Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expanding average temperature of same previous months\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['tempC_exp_mean_per_month'] = 0\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_temp = np.round(yr_df[yr_df['date_month']==mnth]['avg_outside_tempC'].values, 2)\n",
    "        month_dict[mnth].append(mnth_temp)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0])\n",
    "    # remove last value as will be for 2019\n",
    "    month_dict[m].pop(-1)\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "    \n",
    "# insert expanding means\n",
    "for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "    mnth_idx = monthly_train_features[monthly_train_features['date_month']==mnth].index.tolist()\n",
    "    # remove 1st index if there are 6 indices, so we start from 2014\n",
    "    if len(mnth_idx) == 6:\n",
    "        mnth_idx.pop(0)\n",
    "    for idx in range(len(mnth_idx)):\n",
    "        monthly_train_features.loc[mnth_idx[idx], 'tempC_exp_mean_per_month'] = month_dict[mnth][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:11:38.988220Z",
     "start_time": "2021-05-13T11:11:38.910115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expanding average gas usage per of same previous months\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['gas_used_M3'] = monthly_train_target\n",
    "monthly_train_features['gas_used_exp_mean_per_month'] = 0\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_gas = np.round(yr_df[yr_df['date_month']==mnth]['gas_used_M3'].values, 2)\n",
    "        month_dict[mnth].append(mnth_gas)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-2,5)) # add some randomness so its not exact\n",
    "    # remove last value as will be for 2019\n",
    "    month_dict[m].pop(-1)\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "    \n",
    "# insert expanding means\n",
    "for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "    mnth_idx = monthly_train_features[monthly_train_features['date_month']==mnth].index.tolist()\n",
    "    # remove 1st index if there are 6 indices, so we start from 2014\n",
    "    if len(mnth_idx) == 6:\n",
    "        mnth_idx.pop(0)\n",
    "    for idx in range(len(mnth_idx)):\n",
    "        monthly_train_features.loc[mnth_idx[idx], 'gas_used_exp_mean_per_month'] = month_dict[mnth][idx]\n",
    "monthly_train_features.drop(columns=['gas_used_M3'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:11:48.943742Z",
     "start_time": "2021-05-13T11:11:48.923721Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cyclical nature of quarter\n",
    "monthly_train_features, _ = load_train_data()\n",
    "\n",
    "monthly_train_features['quarter_sin'] = np.sin(2*np.pi*monthly_train_features.quarter/4)\n",
    "monthly_train_features['quarter_cos'] = np.cos(2*np.pi*monthly_train_features.quarter/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:12:06.803276Z",
     "start_time": "2021-05-13T11:12:06.748241Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of different hourly recordings of heating pump on per month\n",
    "monthly_train_features, _ = load_train_data()\n",
    "\n",
    "pump_on_dwn = downstairs_heating[downstairs_heating['heating_downstairs_on']==1]\n",
    "daily_hours = pump_on_dwn.groupby(by=['date_year','date_month','date_day']).agg({'hour_':['nunique']})\n",
    "daily_hours.reset_index(inplace=True)\n",
    "daily_hours.columns = ['date_year','date_month','date_day','num_hourly_recordings_dwn']\n",
    "daily_hours = daily_hours.groupby(by=['date_year','date_month']).agg({'num_hourly_recordings_dwn':['sum']})\n",
    "daily_hours.reset_index(inplace=True)\n",
    "daily_hours.columns = ['date_year','date_month','num_hourly_recordings_dwn']\n",
    "daily_hours = daily_hours[daily_hours['date_year']<2019] # create train data\n",
    "\n",
    "pump_on_up = upstairs_heating[upstairs_heating['heating_upstairs_on']==1]\n",
    "daily_hours_up = pump_on_up.groupby(by=['date_year','date_month','date_day']).agg({'hour_':['nunique']})\n",
    "daily_hours_up.reset_index(inplace=True)\n",
    "daily_hours_up.columns = ['date_year','date_month','date_day','num_hourly_recordings_up']\n",
    "daily_hours_up = daily_hours_up.groupby(by=['date_year','date_month']).agg({'num_hourly_recordings_up':['sum']})\n",
    "daily_hours_up.reset_index(inplace=True)\n",
    "daily_hours_up.columns = ['date_year','date_month','num_hourly_recordings_up']\n",
    "daily_hours_up = daily_hours_up[daily_hours_up['date_year']<2019] # create train data\n",
    "\n",
    "daily_hours['num_hourly_recordings_up'] = daily_hours_up['num_hourly_recordings_up']\n",
    "monthly_train_features['heat_on_dwn_num_hourly_recordings'] = daily_hours['num_hourly_recordings_dwn']\n",
    "monthly_train_features['heat_on_up_num_hourly_recordings'] = daily_hours['num_hourly_recordings_up']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:12:18.190762Z",
     "start_time": "2021-05-13T11:12:18.115741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expanding number of hourly recordings of heat downstairs per month of same previous months\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['hodnhr_exp_mean_per_month'] = 0\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_recs = np.round(yr_df[yr_df['date_month']==mnth]['heat_on_dwn_num_hourly_recordings'].values, 2)\n",
    "        month_dict[mnth].append(mnth_recs)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-2,5)) # add some randomness so its not exact\n",
    "    # remove last value as will be for 2019\n",
    "    month_dict[m].pop(-1)\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "    \n",
    "# insert expanding means\n",
    "for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "    mnth_idx = monthly_train_features[monthly_train_features['date_month']==mnth].index.tolist()\n",
    "    # remove 1st index if there are 6 indices, so we start from 2014\n",
    "    if len(mnth_idx) == 6:\n",
    "        mnth_idx.pop(0)\n",
    "    for idx in range(len(mnth_idx)):\n",
    "        monthly_train_features.loc[mnth_idx[idx], 'hodnhr_exp_mean_per_month'] = month_dict[mnth][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:12:49.244834Z",
     "start_time": "2021-05-13T11:12:49.161787Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expanding number of hourly recordings of heat upstairs per month of same previous months\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['hounhr_exp_mean_per_month'] = 0\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_recs = np.round(yr_df[yr_df['date_month']==mnth]['heat_on_up_num_hourly_recordings'].values, 2)\n",
    "        month_dict[mnth].append(mnth_recs)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-2,5)) # add some randomness so its not exact\n",
    "    # remove last value as will be for 2019\n",
    "    month_dict[m].pop(-1)\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "    \n",
    "# insert expanding means\n",
    "for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "    mnth_idx = monthly_train_features[monthly_train_features['date_month']==mnth].index.tolist()\n",
    "    # remove 1st index if there are 6 indices, so we start from 2014\n",
    "    if len(mnth_idx) == 6:\n",
    "        mnth_idx.pop(0)\n",
    "    for idx in range(len(mnth_idx)):\n",
    "        monthly_train_features.loc[mnth_idx[idx], 'hounhr_exp_mean_per_month'] = month_dict[mnth][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:13:13.486891Z",
     "start_time": "2021-05-13T11:13:13.125699Z"
    }
   },
   "outputs": [],
   "source": [
    "# avg downstairs temp\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "lr = living_room_temp.groupby(by=['date_year','date_month']).agg({'living_temp_C':['mean']})\n",
    "k = kitchen_temp.groupby(by=['date_year','date_month']).agg({'kitchen_temp_C':['mean']})\n",
    "st = storage_temp.groupby(by=['date_year','date_month']).agg({'storage_temp_C':['mean']})\n",
    "lr['kitchen_temp_C'] = k['kitchen_temp_C']['mean']\n",
    "lr['storage_temp_C'] = st['storage_temp_C']['mean']\n",
    "lr.reset_index(inplace=True)\n",
    "# get average of rooms\n",
    "avg_temp_dwn = []\n",
    "for i in range(len(lr)):\n",
    "    lt = lr['living_temp_C']['mean'][i]\n",
    "    kt = lr['kitchen_temp_C'][i]\n",
    "    st = lr['storage_temp_C'][i]\n",
    "    mean_temp = np.mean([lt,kt,st])\n",
    "    avg_temp_dwn.append(mean_temp)\n",
    "\n",
    "lr['avg_downstairs_tempC'] = avg_temp_dwn\n",
    "# create training data\n",
    "lr = lr[lr['date_year']<2019]\n",
    "lr = lr.iloc[1:,:].reset_index(drop=True)\n",
    "monthly_train_features['avg_downstairs_tempC'] = np.round(lr['avg_downstairs_tempC'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:13:21.797634Z",
     "start_time": "2021-05-13T11:13:21.245696Z"
    }
   },
   "outputs": [],
   "source": [
    "# avg upstairs temp\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "b1 = bedroom1_temp.groupby(by=['date_year','date_month']).agg({'bedroom1_temp_C':['mean']})\n",
    "b2 = bedroom2_temp.groupby(by=['date_year','date_month']).agg({'bedroom2_temp_C':['mean']})\n",
    "b3 = bedroom3_temp.groupby(by=['date_year','date_month']).agg({'bedroom3_temp_C':['mean']})\n",
    "bth = bathroom_temp.groupby(by=['date_year','date_month']).agg({'bathroom_temp_C':['mean']})\n",
    "dsk = desk_temp.groupby(by=['date_year','date_month']).agg({'desk_temp_C':['mean']})\n",
    "b1['bedroom2_temp_C'] = b2['bedroom2_temp_C']['mean']\n",
    "b1['bedroom3_temp_C'] = b3['bedroom3_temp_C']['mean']\n",
    "b1['bathroom_temp_C'] = bth['bathroom_temp_C']['mean']\n",
    "b1['desk_temp_C'] = dsk['desk_temp_C']['mean']\n",
    "b1.reset_index(inplace=True)\n",
    "# get average of rooms\n",
    "avg_temp_up = []\n",
    "for i in range(len(b1)):\n",
    "    b1t = b1['bedroom1_temp_C']['mean'][i]\n",
    "    b2t = b1['bedroom2_temp_C'][i]\n",
    "    b3t = b1['bedroom3_temp_C'][i]\n",
    "    btht = b1['bathroom_temp_C'][i]\n",
    "    dskt = b1['desk_temp_C'][i]\n",
    "    mean_temp = np.mean([b1t,b2t,b3t,btht,dskt])\n",
    "    avg_temp_up.append(mean_temp)\n",
    "\n",
    "b1['avg_upstairs_tempC'] = avg_temp_up\n",
    "# create training data\n",
    "b1 = b1[b1['date_year']<2019]\n",
    "b1 = b1.iloc[1:,:].reset_index(drop=True)\n",
    "monthly_train_features['avg_upstairs_tempC'] = np.round(b1['avg_upstairs_tempC'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:13:36.490568Z",
     "start_time": "2021-05-13T11:13:36.395527Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding mean of avg downstairs temp per month\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['avg_dwn_temp_exp_mean_per_month'] = 0\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_temp = np.round(yr_df[yr_df['date_month']==mnth]['avg_downstairs_tempC'].values, 2)\n",
    "        month_dict[mnth].append(mnth_temp)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-2,5)) # add some randomness so its not exact\n",
    "    # remove last value as will be for 2019\n",
    "    month_dict[m].pop(-1)\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "    \n",
    "# insert expanding means\n",
    "for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "    mnth_idx = monthly_train_features[monthly_train_features['date_month']==mnth].index.tolist()\n",
    "    # remove 1st index if there are 6 indices, so we start from 2014\n",
    "    if len(mnth_idx) == 6:\n",
    "        mnth_idx.pop(0)\n",
    "    for idx in range(len(mnth_idx)):\n",
    "        monthly_train_features.loc[mnth_idx[idx], 'avg_dwn_temp_exp_mean_per_month'] = month_dict[mnth][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:13:49.906737Z",
     "start_time": "2021-05-13T11:13:49.820651Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding mean of avg upstairs temp per month\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['avg_up_temp_exp_mean_per_month'] = 0\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_temp = np.round(yr_df[yr_df['date_month']==mnth]['avg_upstairs_tempC'].values, 2)\n",
    "        month_dict[mnth].append(mnth_temp)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-2,5)) # add some randomness so its not exact\n",
    "    # remove last value as will be for 2019\n",
    "    month_dict[m].pop(-1)\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "    \n",
    "# insert expanding means\n",
    "for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "    mnth_idx = monthly_train_features[monthly_train_features['date_month']==mnth].index.tolist()\n",
    "    # remove 1st index if there are 6 indices, so we start from 2014\n",
    "    if len(mnth_idx) == 6:\n",
    "        mnth_idx.pop(0)\n",
    "    for idx in range(len(mnth_idx)):\n",
    "        monthly_train_features.loc[mnth_idx[idx], 'avg_up_temp_exp_mean_per_month'] = month_dict[mnth][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:13:56.671349Z",
     "start_time": "2021-05-13T11:13:56.653305Z"
    }
   },
   "outputs": [],
   "source": [
    "# upstairs/downstairs heat on num hourly recordings lag features\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['hodnhr_lag12'] = monthly_train_features['heat_on_dwn_num_hourly_recordings'].shift(12)\n",
    "monthly_train_features['hodnhr_lag1'] = monthly_train_features['heat_on_dwn_num_hourly_recordings'].shift(1)\n",
    "monthly_train_features['hounhr_lag12'] = monthly_train_features['heat_on_up_num_hourly_recordings'].shift(12)\n",
    "monthly_train_features['hounhr_lag1'] = monthly_train_features['heat_on_up_num_hourly_recordings'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:14:02.073993Z",
     "start_time": "2021-05-13T11:14:02.052957Z"
    }
   },
   "outputs": [],
   "source": [
    "# avg upstairs/downstairs temp lag features\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['avg_downstairs_temp_lag12'] = monthly_train_features['avg_downstairs_tempC'].shift(12)\n",
    "monthly_train_features['avg_downstairs_temp_lag1'] = monthly_train_features['avg_downstairs_tempC'].shift(1)\n",
    "monthly_train_features['avg_upstairs_temp_lag12'] = monthly_train_features['avg_upstairs_tempC'].shift(12)\n",
    "monthly_train_features['avg_upstairs_temp_lag1'] = monthly_train_features['avg_upstairs_tempC'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:14:13.734707Z",
     "start_time": "2021-05-13T11:14:13.716680Z"
    }
   },
   "outputs": [],
   "source": [
    "# house avg temp, lag\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['avg_house_tempC'] = pd.Series(np.mean(np.stack((monthly_train_features.avg_downstairs_tempC,monthly_train_features.avg_upstairs_tempC)), axis=0))\n",
    "monthly_train_features['avg_house_tempC_lag12'] = monthly_train_features['avg_house_tempC'].shift(12)\n",
    "monthly_train_features['avg_house_tempC_lag1'] = monthly_train_features['avg_house_tempC'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:14:24.013655Z",
     "start_time": "2021-05-13T11:14:23.990654Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding mean ratio of outside temp and gas used\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['exp_mean_ratio_outsidetemp_gas_used'] = (monthly_train_features['tempC_exp_mean_per_month']+1) /(monthly_train_features['gas_used_exp_mean_per_month']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:14:38.985118Z",
     "start_time": "2021-05-13T11:14:38.903118Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding mean of avg house temp per month\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['avg_house_temp_exp_mean_per_month'] = 0\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_temp = np.round(yr_df[yr_df['date_month']==mnth]['avg_house_tempC'].values, 2)\n",
    "        month_dict[mnth].append(mnth_temp)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-2,5)) # add some randomness so its not exact\n",
    "    # remove last value as will be for 2019\n",
    "    month_dict[m].pop(-1)\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "    \n",
    "# insert expanding means\n",
    "for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "    mnth_idx = monthly_train_features[monthly_train_features['date_month']==mnth].index.tolist()\n",
    "    # remove 1st index if there are 6 indices, so we start from 2014\n",
    "    if len(mnth_idx) == 6:\n",
    "        mnth_idx.pop(0)\n",
    "    for idx in range(len(mnth_idx)):\n",
    "        monthly_train_features.loc[mnth_idx[idx], 'avg_house_temp_exp_mean_per_month'] = month_dict[mnth][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:14:47.278967Z",
     "start_time": "2021-05-13T11:14:47.260007Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding mean ratio of house temp and gas used\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['exp_mean_ratio_housetemp_gas_used'] = (monthly_train_features['avg_house_temp_exp_mean_per_month']+1) /(monthly_train_features['gas_used_exp_mean_per_month']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:15:12.229408Z",
     "start_time": "2021-05-13T11:15:12.184405Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding mean ratio heat on downstairs number of hourly recordings per month and gas used\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['exp_mean_ratio_hodnhr_gas_used'] = (monthly_train_features['hodnhr_exp_mean_per_month']+1) /(monthly_train_features['gas_used_exp_mean_per_month']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:15:24.510243Z",
     "start_time": "2021-05-13T11:15:24.490235Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding mean ratio heat on upstairs number of hourly recordings per month and gas used\n",
    "monthly_train_features, _ = load_train_data()\n",
    "monthly_train_features['exp_mean_ratio_hounhr_gas_used'] = (monthly_train_features['hounhr_exp_mean_per_month']+1) /(monthly_train_features['gas_used_exp_mean_per_month']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:26:10.491853Z",
     "start_time": "2021-05-13T11:26:10.449846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# knn feature engineering with best features \n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "clean_cols = [\n",
    "    'month_sin','month_cos','roll_mean_targ1','exp_mean_ratio_outsidetemp_gas_used','exp_mean_ratio_housetemp_gas_used']\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
    "knn.fit(monthly_train_features[clean_cols].iloc[1:,:], monthly_train_target[1:])\n",
    "pred = knn.predict(monthly_train_features[clean_cols].iloc[1:,:])\n",
    "\n",
    "pred = np.insert(pred,0,0)\n",
    "monthly_train_features['knn_local_knowledge'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T12:35:57.011044Z",
     "start_time": "2021-05-23T12:35:56.966010Z"
    }
   },
   "outputs": [],
   "source": [
    "# month sin/cos combinations\n",
    "monthly_train_features,_ = load_train_data()\n",
    "monthly_train_features['roll_mean_targ1*mnth_sin'] = monthly_train_features['month_sin']*monthly_train_features['roll_mean_targ1']\n",
    "monthly_train_features['roll_mean_targ1*mnth_cos'] = monthly_train_features['month_cos']*monthly_train_features['roll_mean_targ1']\n",
    "\n",
    "monthly_train_features['exp_mean_ratio_outsidetemp_gas_used*mnth_sin'] = monthly_train_features['month_sin']*monthly_train_features['exp_mean_ratio_outsidetemp_gas_used']\n",
    "monthly_train_features['exp_mean_ratio_outsidetemp_gas_used*mnth_cos'] = monthly_train_features['month_cos']*monthly_train_features['exp_mean_ratio_outsidetemp_gas_used']\n",
    "\n",
    "monthly_train_features['exp_mean_ratio_housetemp_gas_used*mnth_sin'] = monthly_train_features['month_sin']*monthly_train_features['exp_mean_ratio_housetemp_gas_used']\n",
    "monthly_train_features['exp_mean_ratio_housetemp_gas_used*mnth_cos'] = monthly_train_features['month_cos']*monthly_train_features['exp_mean_ratio_housetemp_gas_used']\n",
    "\n",
    "monthly_train_features['knn_local_knowledge*mnth_sin'] = monthly_train_features['month_sin']*monthly_train_features['knn_local_knowledge']\n",
    "monthly_train_features['knn_local_knowledge*mnth_cos'] = monthly_train_features['month_cos']*monthly_train_features['knn_local_knowledge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T20:13:51.524111Z",
     "start_time": "2021-05-26T20:13:51.472571Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding average gas usage per season\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['gas_used_M3'] = monthly_train_target\n",
    "monthly_train_features['gas_used_exp_mean_per_season'] = 0\n",
    "season_dict = {\n",
    "    'winter': [],\n",
    "    'spring': [],\n",
    "    'summer': [],\n",
    "    'autumn': [],\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for s in sorted(monthly_train_features['season'].unique()):\n",
    "        s_gas = np.round(np.sum(yr_df[yr_df['season']==s]['gas_used_M3'].values), 2)\n",
    "        season_dict[s].append(s_gas)\n",
    "# remove empty array and expand mean\n",
    "for s in season_dict:\n",
    "    # remove 1st entry as not full season\n",
    "    season_dict[s].pop(0)\n",
    "    # insert 1st value twice then add randomness within standard deviation so we dont have a full season\n",
    "    season_dict[s].insert(0, np.round(season_dict[s][0] + np.random.uniform(-np.std(season_dict[s]),np.std(season_dict[s])), 2)) # add some randomness so its not exact\n",
    "    # remove last value as will be for 2019\n",
    "    season_dict[s].pop(-1)\n",
    "    # convert to pd series to use expanding\n",
    "    season_dict[s] = pd.Series(season_dict[s])\n",
    "    season_dict[s] = np.round(season_dict[s].expanding(1).mean(), 2)\n",
    "    \n",
    "# insert expanding means\n",
    "for s in sorted(monthly_train_features['season'].unique()):\n",
    "    for i in range(5):  \n",
    "        yr_df = monthly_train_features[monthly_train_features['date_year']==2014+i]\n",
    "        s_idx = yr_df[yr_df['season']==s].index.tolist()\n",
    "        monthly_train_features.loc[s_idx, 'gas_used_exp_mean_per_season'] = season_dict[s][i]\n",
    "monthly_train_features.drop(columns=['gas_used_M3'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T05:20:51.396482Z",
     "start_time": "2021-05-27T05:20:51.384446Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### rolling target mean of 1 with lag of 12 months\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "roll_targ = monthly_train_target.rolling(window=2).mean()\n",
    "roll_lag = roll_targ.shift(12)\n",
    "monthly_train_features['roll_mean_targ1_lag_12'] = roll_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T06:04:33.775203Z",
     "start_time": "2021-05-27T06:04:33.748163Z"
    }
   },
   "outputs": [],
   "source": [
    "# boxcox transforms\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "og = boxcox(monthly_train_features['exp_mean_ratio_outsidetemp_gas_used'], lmbda=0.04347125404618559)\n",
    "hg = boxcox(monthly_train_features['exp_mean_ratio_housetemp_gas_used'], lmbda=-0.04065702816262846)\n",
    "\n",
    "monthly_train_features['exp_mean_ratio_outsidetemp_gas_used_bxcx'] = og\n",
    "monthly_train_features['exp_mean_ratio_housetemp_gas_used_bxcx'] = hg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T06:05:30.759950Z",
     "start_time": "2021-05-27T06:05:30.743012Z"
    }
   },
   "outputs": [],
   "source": [
    "#save_data(monthly_train_features, 'gas_consumption_monthly_train_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:27:06.834456Z",
     "start_time": "2021-05-13T11:27:06.814464Z"
    },
    "scrolled": true
   },
   "source": [
    "### Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T09:50:16.941856Z",
     "start_time": "2021-05-31T09:50:16.872266Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expanding average gas usage per of same previous months\n",
    "monthly_test_features, monthly_test_target = load_test_data()\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "monthly_train_features['gas_used_M3'] = monthly_train_target\n",
    "\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_gas = np.round(yr_df[yr_df['date_month']==mnth]['gas_used_M3'].values, 2)\n",
    "        month_dict[mnth].append(mnth_gas)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-2,5)) # add some randomness so its not exact\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "\n",
    "exp_mean_2019 = [month_dict[m][5] for m in month_dict]\n",
    "\n",
    "monthly_test_features['gas_used_exp_mean_per_month'] = exp_mean_2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:52:36.026002Z",
     "start_time": "2021-05-31T10:52:35.957979Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding average temperature of same previous months\n",
    "monthly_test_features, _ = load_test_data()\n",
    "monthly_train_features, _ = load_train_data()\n",
    "\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_temp = np.round(yr_df[yr_df['date_month']==mnth]['avg_outside_tempC'].values, 2)\n",
    "        month_dict[mnth].append(mnth_temp)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-np.std(month_dict[m]), np.std(month_dict[m])))\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "\n",
    "exp_temp_2019 = [month_dict[m][5] for m in month_dict]\n",
    "\n",
    "monthly_test_features['tempC_exp_mean_per_month'] = exp_temp_2019\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:59:50.016459Z",
     "start_time": "2021-05-31T10:59:49.942448Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expanding mean of avg house temp per month\n",
    "monthly_test_features,_ = load_test_data()\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "\n",
    "month_dict = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: [],\n",
    "    6: [],\n",
    "    7: [],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: [],\n",
    "    11: [],\n",
    "    12: []\n",
    "}\n",
    "for yr in sorted(monthly_train_features['date_year'].unique()):\n",
    "    yr_df = monthly_train_features[monthly_train_features['date_year']==yr]\n",
    "    for mnth in sorted(monthly_train_features['date_month'].unique()):\n",
    "        mnth_temp = np.round(yr_df[yr_df['date_month']==mnth]['avg_house_tempC'].values, 2)\n",
    "        month_dict[mnth].append(mnth_temp)\n",
    "# remove empty array and expand mean\n",
    "for m in month_dict:\n",
    "    # remove empty array\n",
    "    month_dict[m] = [x for x in month_dict[m] if x.size > 0]\n",
    "    # insert 1st value twice so we dont have a nan\n",
    "    if len(month_dict[m]) < 6:\n",
    "        month_dict[m].insert(0, month_dict[m][0] + np.random.uniform(-np.std(month_dict[m]), np.std(month_dict[m]))) # add some randomness so its not exact\n",
    "    # convert to pd series to use expanding\n",
    "    month_dict[m] = pd.Series(month_dict[m])\n",
    "    month_dict[m] = np.round(month_dict[m].expanding(1).mean(), 2)\n",
    "    \n",
    "exp_htemp_2019 = [month_dict[m][5] for m in month_dict]\n",
    "\n",
    "monthly_test_features['avg_house_temp_exp_mean_per_month'] = exp_htemp_2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:04:48.697193Z",
     "start_time": "2021-05-31T11:04:48.675186Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expanding mean ratio of outside temp and gas usage\n",
    "monthly_test_features, _ = load_test_data()\n",
    "monthly_test_features['exp_mean_ratio_outsidetemp_gas_used'] = (monthly_test_features['tempC_exp_mean_per_month']) /(monthly_test_features['gas_used_exp_mean_per_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:07:12.482498Z",
     "start_time": "2021-05-31T11:07:12.448469Z"
    }
   },
   "outputs": [],
   "source": [
    "# expanding mean ratio of house temp and gas used\n",
    "monthly_test_features, _ = load_test_data()\n",
    "monthly_test_features['exp_mean_ratio_housetemp_gas_used'] = (monthly_test_features['avg_house_temp_exp_mean_per_month']) /(monthly_test_features['gas_used_exp_mean_per_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:20:42.033967Z",
     "start_time": "2021-05-31T11:20:41.990959Z"
    }
   },
   "outputs": [],
   "source": [
    "# knn feature engineering with best features \n",
    "monthly_test_features, _ = load_test_data()\n",
    "monthly_train_features, monthly_train_target = load_train_data()\n",
    "clean_cols = [\n",
    "    'month_sin','month_cos','exp_mean_ratio_outsidetemp_gas_used','exp_mean_ratio_housetemp_gas_used'] #'roll_mean_targ1'\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
    "knn.fit(monthly_train_features[clean_cols].iloc[1:,:], monthly_train_target[1:])\n",
    "pred = knn.predict(monthly_test_features[clean_cols])\n",
    "\n",
    "monthly_test_features['knn_local_knowledge'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T11:21:46.305417Z",
     "start_time": "2021-05-31T11:21:46.296415Z"
    }
   },
   "outputs": [],
   "source": [
    "#save_data(monthly_test_features, 'gas_consumption_monthly_test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Baseline -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no outliers present in the data we can use RMSE to evaluate our models, as mentioned we are unable to obtain gaussian  like distributed features, so our focus will be on tree based, support vector and boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T14:34:36.562946Z",
     "start_time": "2021-05-10T14:34:36.544944Z"
    }
   },
   "outputs": [],
   "source": [
    "# target average baseline score full data from 2014\n",
    "_, monthly_train_target = load_train_data()\n",
    "baseline_rmse = monthly_train_target[2:].values.mean()\n",
    "cost = cost_of_gas(baseline_rmse, 2019)\n",
    "print(f'Baseline RMSE: {baseline_rmse},    Cost: {cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T12:42:33.619262Z",
     "start_time": "2021-05-13T12:42:33.575276Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# baseline with cv -- full train data from 2014\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "monthly_train_features = monthly_train_features[['month_sin','month_cos']]\n",
    "model = DecisionTreeRegressor(max_depth=3, min_samples_split=3, max_features='auto', random_state=42)\n",
    "scores = expanding_window_cv(monthly_train_features, monthly_train_target, model, fold_size=12, init_fold=12, scoring='rmse')\n",
    "avg_cost = cost_of_gas(np.mean(scores), 2019)\n",
    "print(f'Simple model baseline\\nAvg RMSE: {np.mean(scores)},   Std: {np.std(scores)}\\nAvg Cost: {avg_cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Modelling -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T13:47:56.811301Z",
     "start_time": "2021-05-30T13:47:52.993468Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test models\n",
    "models, names = get_ml_models()\n",
    "mod_names = []\n",
    "mod_rmse = []\n",
    "mod_rmse_std = []\n",
    "mod_mae = []\n",
    "mod_mape = []\n",
    "mod_r2 = []\n",
    "\n",
    "\n",
    "for i in range(len(models)):\n",
    "    if names[i] in ['SVR','MARS','LGAM']:\n",
    "        pipe = Pipeline(steps =[\n",
    "            ('sc', StandardScaler()),\n",
    "            ('pca', PCA(n_components=2, whiten=True)),\n",
    "            ('model', models[i])\n",
    "        ])\n",
    "        monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "    elif names[i] in ['LR','R','SGD']:\n",
    "        pipe = Pipeline(steps =[\n",
    "            ('sc', StandardScaler()),\n",
    "            ('pca', PCA(n_components=2, whiten=True)),\n",
    "            ('model', models[i])\n",
    "        ])\n",
    "        monthly_train_features, monthly_train_target = load_clean_train_data_lm()\n",
    "    else:\n",
    "        pipe = Pipeline(steps=[\n",
    "            ('sc', StandardScaler()),\n",
    "            ('model', models[i])\n",
    "        ])\n",
    "        monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "    rmse_scores, mae_scores, mape_scores, r2_scores, _ = expanding_window_cv(monthly_train_features, np.array(monthly_train_target).reshape(-1,), pipe, fold_size=12, init_fold=12, scoring='rmse')\n",
    "    mod_names.append(names[i])\n",
    "    mod_rmse.append(np.mean(rmse_scores))\n",
    "    mod_rmse_std.append(np.std(rmse_scores))\n",
    "    mod_mae.append(np.mean(mae_scores))\n",
    "    mod_mape.append(np.mean(mape_scores))\n",
    "    mod_r2.append(np.mean(r2_scores))\n",
    "\n",
    "model_df = pd.DataFrame({'model': mod_names, 'Avg RMSE': mod_rmse, 'RMSE Std': mod_rmse_std, 'Avg MAE': mod_mae, 'Avg MAPE': mod_mape, 'Avg R2': mod_r2}).sort_values(by='Avg RMSE', ascending=True).reset_index(drop=True)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Feature Selection -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T14:34:21.294690Z",
     "start_time": "2021-05-30T14:34:10.259569Z"
    }
   },
   "outputs": [],
   "source": [
    "# best subset extra trees\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "et_model = ExtraTreesRegressor(\n",
    "    n_estimators=100, max_depth=3, min_samples_split=3, max_features='auto', n_jobs=-1, random_state=42)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', et_model)\n",
    "    ]\n",
    ")\n",
    "best_combo = None\n",
    "best_score = None\n",
    "for i in range(5):\n",
    "    print(f'Processing combinations of size {i+1}')\n",
    "    combos = combinations(monthly_train_features.columns, i+1)\n",
    "    for features in combos:\n",
    "        mt_f = monthly_train_features[list(features)]\n",
    "        rmse_scores, _, _, _, _ = expanding_window_cv(mt_f, np.array(monthly_train_target).reshape(-1,), pipe, fold_size=12, init_fold=12)\n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        if best_score == None:\n",
    "            best_score = avg_rmse\n",
    "            best_combo = features\n",
    "        elif best_score != None:\n",
    "            if avg_rmse < best_score:\n",
    "                best_score = avg_rmse\n",
    "                best_combo = features\n",
    "\n",
    "print(f'Extra Trees:\\nBest RMSE: {best_score}\\nBest Feature Combination: {best_combo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T14:38:16.871194Z",
     "start_time": "2021-05-30T14:38:12.342231Z"
    }
   },
   "outputs": [],
   "source": [
    "# best subset adaboost\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "ab_model = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', ab_model)\n",
    "    ]\n",
    ")\n",
    "best_combo = None\n",
    "best_score = None\n",
    "for i in range(5):\n",
    "    print(f'Processing combinations of size {i+1}')\n",
    "    combos = combinations(monthly_train_features.columns, i+1)\n",
    "    for features in combos:\n",
    "        mt_f = monthly_train_features[list(features)]\n",
    "        rmse_scores, _, _, _, _ = expanding_window_cv(mt_f, np.array(monthly_train_target).reshape(-1,), pipe, fold_size=12, init_fold=12)\n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        if best_score == None:\n",
    "            best_score = avg_rmse\n",
    "            best_combo = features\n",
    "        elif best_score != None:\n",
    "            if avg_rmse < best_score:\n",
    "                best_score = avg_rmse\n",
    "                best_combo = features\n",
    "\n",
    "print(f'Adaboost:\\nBest RMSE: {best_score}\\nBest Feature Combination: {best_combo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T14:40:15.978923Z",
     "start_time": "2021-05-30T14:40:13.529924Z"
    }
   },
   "outputs": [],
   "source": [
    "# best subset gradient boosting\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.1, n_estimators=100, min_samples_split=3, max_features='auto', random_state=42)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', gb_model)\n",
    "    ]\n",
    ")\n",
    "best_combo = None\n",
    "best_score = None\n",
    "for i in range(5):\n",
    "    print(f'Processing combinations of size {i+1}')\n",
    "    combos = combinations(monthly_train_features.columns, i+1)\n",
    "    for features in combos:\n",
    "        mt_f = monthly_train_features[list(features)]\n",
    "        rmse_scores, _, _, _, _ = expanding_window_cv(mt_f, np.array(\n",
    "            monthly_train_target).reshape(-1,), pipe, fold_size=12, init_fold=12)\n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        if best_score == None:\n",
    "            best_score = avg_rmse\n",
    "            best_combo = features\n",
    "        elif best_score != None:\n",
    "            if avg_rmse < best_score:\n",
    "                best_score = avg_rmse\n",
    "                best_combo = features\n",
    "\n",
    "print(\n",
    "    f'Gradient Boosting:\\nBest RMSE: {best_score}\\nBest Feature Combination: {best_combo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:09:45.649560Z",
     "start_time": "2021-05-13T16:09:45.634462Z"
    }
   },
   "source": [
    "## - Hyperparameter Tuning - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:01:45.955739Z",
     "start_time": "2021-05-30T14:47:09.467302Z"
    }
   },
   "outputs": [],
   "source": [
    "# extra trees regressor\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "monthly_train_features = monthly_train_features[['month_sin','month_cos','exp_mean_ratio_housetemp_gas_used', 'knn_local_knowledge']]\n",
    "\n",
    "et_model = ExtraTreesRegressor(\n",
    "    n_estimators=100, max_depth=3, min_samples_split=3, max_features='auto', n_jobs=-1, random_state=42)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', et_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def et_objective(params):\n",
    "    pipe.set_params(**params)\n",
    "    scores, _, _, _, _ = expanding_window_cv(\n",
    "        monthly_train_features, np.array(monthly_train_target).reshape(-1,), pipe, fold_size=12, init_fold=12, scoring='rmse')\n",
    "    avg_loss = np.mean(scores)\n",
    "    return {'loss': avg_loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "# define parameters\n",
    "param_space = {\n",
    "    'model__n_estimators': scope.int(hp.quniform('model__n_estimators', 50, 1000, 10)),\n",
    "    'model__max_depth': scope.int(hp.quniform('model__max_depth', 1, 8, 1)),\n",
    "    'model__min_samples_split': scope.int(hp.quniform('model__min_samples_split', 2, 12, 1)),\n",
    "    'model__min_samples_leaf': scope.int(hp.quniform('model__min_samples_leaf', 1, 12, 1)),\n",
    "    'model__max_features': hp.choice('model__max_features', ['auto','sqrt','log2']),\n",
    "    'model__min_impurity_decrease': hp.uniform('model__min_impurity_decrease', 0.0, 0.6),\n",
    "    'model__bootstrap': hp.choice('model__bootstrap', [True, False]),\n",
    "    'model__ccp_alpha': hp.uniform('model__ccp_alpha', 0.0, 0.2)\n",
    "}\n",
    "\n",
    "# optimize model params\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=et_objective, space=param_space,\n",
    "                   algo=tpe.suggest, max_evals=1000, trials=trials)\n",
    "\n",
    "best_param_space = space_eval(param_space, best_params)\n",
    "print(f'Best Parameters:\\n{best_param_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all feats:\n",
    "best loss: 16.453558344937093\n",
    "Best Parameters:\n",
    "{'model__bootstrap': False, 'model__ccp_alpha': 0.12885482457237743, 'model__max_depth': 4, 'model__max_features': 'log2', \n",
    " 'model__min_impurity_decrease': 0.49516117778118646, 'model__min_samples_leaf': 1, 'model__min_samples_split': 3, \n",
    " 'model__n_estimators': 100}\n",
    " \n",
    "drop roll_mean_targ1----\n",
    "best loss: 17.11262925718358\n",
    "Best Parameters:\n",
    "{'model__bootstrap': True, 'model__ccp_alpha': 0.11451043932660587, 'model__max_depth': 3, 'model__max_features': 'auto', \n",
    " 'model__min_impurity_decrease': 0.02605916245722535, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, \n",
    " 'model__n_estimators': 770}\n",
    " \n",
    "one hot encoded months----\n",
    "best loss: 17.3384117745247\n",
    "Best Parameters:\n",
    "{'model__bootstrap': False, 'model__ccp_alpha': 0.14561048691123263, 'model__max_depth': 3, 'model__max_features': 'auto', \n",
    " 'model__min_impurity_decrease': 0.20006503985681634, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, \n",
    " 'model__n_estimators': 500}\n",
    "\n",
    "best features ------\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T15:25:54.678195Z",
     "start_time": "2021-05-30T15:09:13.674191Z"
    }
   },
   "outputs": [],
   "source": [
    "# adaboost\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "monthly_train_features= monthly_train_features[['month_sin', 'month_cos', 'knn_local_knowledge']]\n",
    "ab_model = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', ab_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def ab_objective(params):\n",
    "    pipe.set_params(**params)\n",
    "    scores, _, _, _, _ = expanding_window_cv(\n",
    "        monthly_train_features, np.array(monthly_train_target).reshape(-1,), pipe, fold_size=12, init_fold=12, scoring='rmse')\n",
    "    avg_loss = np.mean(scores)\n",
    "    return {'loss': avg_loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "# define parameters\n",
    "param_space = {\n",
    "    'model__n_estimators': scope.int(hp.quniform('model__n_estimators', 100, 1000, 10)),\n",
    "    'model__learning_rate': hp.uniform('model__learning_rate', 0.5, 2.0),\n",
    "    'model__loss': hp.choice('model__loss', ['linear','square','exponential']),\n",
    "}\n",
    "\n",
    "# optimize model params\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=ab_objective, space=param_space,\n",
    "                   algo=tpe.suggest, max_evals=1000, trials=trials)\n",
    "\n",
    "best_param_space = space_eval(param_space, best_params)\n",
    "print(f'Best Parameters:\\n{best_param_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "best loss: 20.301556359196105\n",
    "Best Parameters:\n",
    "{'model__learning_rate': 1.5898277916109191, 'model__loss': 'square', 'model__n_estimators': 230}\n",
    "\n",
    "best features-----\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T16:02:12.351067Z",
     "start_time": "2021-05-30T15:37:19.155182Z"
    }
   },
   "outputs": [],
   "source": [
    "# gradient boosting\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "monthly_train_features= monthly_train_features[['month_cos', 'knn_local_knowledge']]\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.1, n_estimators=100, min_samples_split=3, max_features='auto', random_state=42)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', gb_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def gb_objective(params):\n",
    "    pipe.set_params(**params)\n",
    "    scores, _, _, _, _ = expanding_window_cv(\n",
    "        monthly_train_features, np.array(monthly_train_target).reshape(-1,), pipe, fold_size=12, init_fold=12, scoring='rmse')\n",
    "    avg_loss = np.mean(scores)\n",
    "    return {'loss': avg_loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "# define parameters\n",
    "param_space = {\n",
    "    'model__n_estimators': scope.int(hp.quniform('model__n_estimators', 100, 1000, 10)),\n",
    "    'model__learning_rate': hp.uniform('model__learning_rate', 0.001, 0.1),\n",
    "    'model__max_depth': scope.int(hp.quniform('model__max_depth', 1, 8, 1)),\n",
    "    'model__min_samples_split': scope.int(hp.quniform('model__min_samples_split', 2, 12, 1)),\n",
    "    'model__min_samples_leaf': scope.int(hp.quniform('model__min_samples_leaf', 1, 12, 1)),\n",
    "    'model__subsample': hp.uniform('model__subsample', 0.5, 0.9),\n",
    "    'model__max_features': hp.choice('model__max_features', ['auto','sqrt','log2']),\n",
    "    'model__min_impurity_decrease': hp.uniform('model__min_impurity_decrease', 0.0, 0.6),\n",
    "    'model__loss': hp.choice('model__loss', ['ls','lad','huber','quantile']),\n",
    "    'model__ccp_alpha': hp.uniform('model__ccp_alpha', 0.0, 0.2)\n",
    "}\n",
    "\n",
    "# optimize model params\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=gb_objective, space=param_space,\n",
    "                   algo=tpe.suggest, max_evals=1000, trials=trials)\n",
    "\n",
    "best_param_space = space_eval(param_space, best_params)\n",
    "print(f'Best Parameters:\\n{best_param_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning summary:\n",
    "\n",
    "    extra trees: RMSE = 16.453558344937093, avg error cost = £7.07\n",
    "\n",
    "    gradient boosting: RMSE = 16.452704707642244, avg error cost = £7.07\n",
    "\n",
    "    adaboost: RMSE = 19.687331563432124, avg error cost = £8.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Model Ensembles -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-30T18:02:47.923214Z",
     "start_time": "2021-05-30T18:02:13.446591Z"
    }
   },
   "outputs": [],
   "source": [
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "\n",
    "et_model = ExtraTreesRegressor(\n",
    "    bootstrap=False, ccp_alpha=0.15110530134134761, max_depth=3, max_features='auto', min_impurity_decrease=0.24850444259073817,\n",
    "    min_samples_leaf=1, min_samples_split=2, n_estimators=80)\n",
    "\n",
    "et_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_sin','month_cos','exp_mean_ratio_housetemp_gas_used', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('et', et_model)\n",
    "])\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    ccp_alpha=0.11899991337691343, learning_rate=0.08512323674014961, loss='lad', max_depth=1, max_features='auto', \n",
    "    min_impurity_decrease=0.3291504192766261, min_samples_leaf=3, min_samples_split=5, n_estimators=930, \n",
    "    subsample=0.6041295730833111)\n",
    "\n",
    "gb_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_cos', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('xgb', gb_model)\n",
    "])\n",
    "\n",
    "\n",
    "estims = [('et_pipe', et_pipe), ('gb_pipe', gb_pipe)]\n",
    "vc= VotingRegressor(\n",
    "    estimators=estims,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "mean_cv_score = []\n",
    "mean_cv_std = []\n",
    "for i in range(10):\n",
    "    scores, _, _, _, _ = expanding_window_cv(\n",
    "            monthly_train_features, np.array(monthly_train_target).reshape(-1,), vc, fold_size=12, init_fold=12)\n",
    "    mean_cv_score.append(np.mean(scores))\n",
    "    mean_cv_std.append(np.std(scores))\n",
    "\n",
    "error_cost = cost_of_gas(np.mean(mean_cv_score), 2019)\n",
    "print(f'Avg Rmse: {np.mean(mean_cv_score)},   Avg Std: {np.mean(mean_cv_std)},    Avg Error Cost: {error_cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T08:59:55.711028Z",
     "start_time": "2021-05-31T08:59:36.616564Z"
    }
   },
   "outputs": [],
   "source": [
    "# extra trees bagging\n",
    "monthly_train_features, monthy_train_target = load_clean_train_data()\n",
    "et = ExtraTreesRegressor(\n",
    "    bootstrap=False, ccp_alpha=0.15110530134134761, max_depth=3, max_features='auto', min_impurity_decrease=0.24850444259073817,\n",
    "    min_samples_leaf=1, min_samples_split=2, n_estimators=80, n_jobs=-1, random_state=42)\n",
    "bgr = BaggingRegressor(base_estimator=et, n_estimators=50, random_state=42, bootstrap=False)\n",
    "bgr_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_sin','month_cos','exp_mean_ratio_housetemp_gas_used', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('model', bgr)\n",
    "])\n",
    "rmse, mae, mape, r2, _ = expanding_window_cv(monthly_train_features, np.array(monthly_train_target).reshape(-1,), bgr_pipe, fold_size=12, init_fold=12)\n",
    "error_cost = cost_of_gas(np.mean(rmse), 2019)\n",
    "print(f'Extra Trees Bagging:\\nAvg RMSE: {np.mean(rmse)}, avg error cost: {error_cost}\\nAvg MAE: {np.mean(mae)}\\nAvg MAPE: {np.mean(mape)}\\nAvg R2: {np.mean(r2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T09:08:38.012707Z",
     "start_time": "2021-05-31T09:06:57.494112Z"
    }
   },
   "outputs": [],
   "source": [
    "monthly_train_features, monthy_train_target = load_clean_train_data()\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    ccp_alpha=0.11899991337691343, learning_rate=0.08512323674014961, loss='lad', max_depth=1, max_features='auto', \n",
    "    min_impurity_decrease=0.3291504192766261, min_samples_leaf=3, min_samples_split=5, n_estimators=930, \n",
    "    subsample=0.6041295730833111, random_state=42)\n",
    "\n",
    "bgr = BaggingRegressor(base_estimator=gb_model, n_estimators=50, random_state=42, bootstrap=False)\n",
    "bgr_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_cos', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('model', bgr)\n",
    "])\n",
    "rmse, mae, mape, r2, _ = expanding_window_cv(monthly_train_features, np.array(monthly_train_target).reshape(-1,), bgr_pipe, fold_size=12, init_fold=12)\n",
    "error_cost = cost_of_gas(np.mean(rmse), 2019)\n",
    "print(f'Gradient Boosting Bagging:\\nAvg RMSE: {np.mean(rmse)}, avg error cost: {error_cost}\\nAvg MAE: {np.mean(mae)}\\nAvg MAPE: {np.mean(mape)}\\nAvg R2: {np.mean(r2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimised Weighted Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T08:36:52.136838Z",
     "start_time": "2021-05-31T08:14:19.133975Z"
    }
   },
   "outputs": [],
   "source": [
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "#tss = TimeSeriesSplit(n_splits = 4)\n",
    "\n",
    "et_model = ExtraTreesRegressor(\n",
    "    bootstrap=False, ccp_alpha=0.15110530134134761, max_depth=3, max_features='auto', min_impurity_decrease=0.24850444259073817,\n",
    "    min_samples_leaf=1, min_samples_split=2, n_estimators=80, n_jobs=-1, random_state=42)\n",
    "\n",
    "et_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_sin','month_cos','exp_mean_ratio_housetemp_gas_used', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('et', et_model)\n",
    "])\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    ccp_alpha=0.11899991337691343, learning_rate=0.08512323674014961, loss='lad', max_depth=1, max_features='auto', \n",
    "    min_impurity_decrease=0.3291504192766261, min_samples_leaf=3, min_samples_split=5, n_estimators=930, \n",
    "    subsample=0.6041295730833111, random_state=42)\n",
    "\n",
    "gb_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_cos', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('gb', gb_model)\n",
    "])\n",
    "\n",
    "ensemble_pipes = [et_pipe, gb_pipe]\n",
    "# bounds for weights\n",
    "weight_bounds = [(0.0, 0.1) for _ in range(len(ensemble_pipes))]\n",
    "# arguments for the loss function\n",
    "search_arg = (ensemble_pipes, monthly_train_features, np.array(monthly_train_target).reshape(-1,))\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(\n",
    "    loss_function, weight_bounds, search_arg, maxiter=100, tol=1e-7)\n",
    "weights = normalise(result['x'])\n",
    "print(f'weights: {weights}')\n",
    "score = opt_ensemble_eval(\n",
    "    ensemble_pipes, weights, monthly_train_features, np.array(monthly_train_target).reshape(-1,))\n",
    "error_cost = cost_of_gas(score, 2019)\n",
    "print(f'Avg optimised ensemble Rmse: {score}, avg error cost: {error_cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Final Model -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T06:31:22.208785Z",
     "start_time": "2021-06-02T06:31:21.405013Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "monthly_test_features, monthly_test_target = load_clean_test_data()\n",
    "# load pipelines\n",
    "# extra trees\n",
    "et_model = ExtraTreesRegressor(\n",
    "    bootstrap=False, ccp_alpha=0.15110530134134761, max_depth=3, max_features='auto', min_impurity_decrease=0.24850444259073817,\n",
    "    min_samples_leaf=1, min_samples_split=2, n_estimators=80, n_jobs=-1, random_state=42)\n",
    "\n",
    "et_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_sin','month_cos','exp_mean_ratio_housetemp_gas_used', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('et', et_model)\n",
    "])\n",
    "# gradient boosting\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    ccp_alpha=0.11899991337691343, learning_rate=0.08512323674014961, loss='lad', max_depth=1, max_features='auto', \n",
    "    min_impurity_decrease=0.3291504192766261, min_samples_leaf=3, min_samples_split=5, n_estimators=930, \n",
    "    subsample=0.6041295730833111, random_state=42)\n",
    "\n",
    "gb_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_cos', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('gb', gb_model)\n",
    "])\n",
    "# fit pipes\n",
    "pipe_names = ['extra_trees', 'gradient_boosting']\n",
    "ensemble_pipes = [et_pipe, gb_pipe]\n",
    "ensemble_fit_pipes = [pipe.fit(monthly_train_features, np.array(monthly_train_target).reshape(-1,)) for pipe in ensemble_pipes]\n",
    "# individual model predictions and save model\n",
    "model_dict = {}\n",
    "for i in range(len(ensemble_fit_pipes)):\n",
    "    mod_preds = ensemble_fit_pipes[i].predict(monthly_test_features)\n",
    "    mod_resids = monthly_test_target.values.flatten() - mod_preds\n",
    "    mod_rmse = mean_squared_error(monthly_test_target, mod_preds, squared=False)\n",
    "    mod_r2 = r2_score(monthly_test_target, mod_preds)\n",
    "    print(f'{pipe_names[i]} model:     RMSE: {mod_rmse},    R2: {mod_r2}')\n",
    "    joblib.dump(ensemble_fit_pipes[i], f'{pipe_names[i]}_home_win_model_pipeline.sav')\n",
    "    model_dict[f'{pipe_names[i]} preds'] = mod_preds\n",
    "    mod_df = pd.DataFrame({'Month': month,\n",
    "                           '2019 Preds': mod_preds,\n",
    "                           '2019 Actual': monthly_test_target,\n",
    "                           'Residuals': mod_resids})\n",
    "    mod_df.to_csv(f'{pipe_names[i]}_predictions_&_residuals_2019', encoding='utf-8', index=False)\n",
    "weights = [0.54812394, 0.45187606] \n",
    "# save weights\n",
    "np.savetxt('optimised_weighted_ensemble_weights.csv',\n",
    "           np.array(weights), delimiter=',') \n",
    "# get ensemble predictions\n",
    "ensemble_preds = opt_ensemble_predictions(ensemble_fit_pipes, weights, monthly_test_features) \n",
    "ens_residuals = monthly_test_target.values.flatten() - ensemble_preds \n",
    "ens_rmse = mean_squared_error(monthly_test_target, ensemble_preds, squared=False)\n",
    "ens_r2 = r2_score(monthly_test_target, ensemble_preds)\n",
    "print(f'Ensemble model:    RMSE: {rmse},    R2: {r2}')\n",
    "# save predictions and residuals\n",
    "month = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "ensemble_df_2019 = pd.DataFrame({'Month': month,\n",
    "                             '2019 Preds': ensemble_preds,\n",
    "                             '2019 Actual': monthly_test_target,\n",
    "                             'Residuals': residuals})\n",
    "ensemble_df_2019.to_csv('ensemble_predictions_&_residuals_2019', encoding='utf-8', index=False)\n",
    "# plot preds\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(range(1,13), monthly_test_target, label='actual', color='red')\n",
    "plt.plot(range(1,13), ensemble_preds, label='ensemble', color='blue')\n",
    "plt.plot(range(1,13), model_dict['extra_trees preds'], label='extra trees', color='green')\n",
    "plt.plot(range(1,13), model_dict['gradient_boosting preds'], label='gradient boosting', color='orange')\n",
    "plt.title('Model Predictions vs Actual')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Gas Used M3')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:19:23.584523Z",
     "start_time": "2021-06-04T13:24:59.701172Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction interval\n",
    "# take each row including target (train data)\n",
    "# draw a row (including target at random) (train data)\n",
    "# store row and replace back\n",
    "# repeat n times for bootstrap resample\n",
    "# fit regression, predict new value (test data)\n",
    "# take a single residual at random from original regression fit, add it to the predicted value and record result\n",
    "# repeat say 1000 times\n",
    "# find the 2.5th and 97.5th percentiles of the results for the prediction interval\n",
    "\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "monthly_test_features, monthly_test_target = load_clean_test_data()\n",
    "# combine train features and target for resample\n",
    "combi_train = pd.concat([monthly_train_features, monthly_train_target], axis=1)\n",
    "# create best model\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    ccp_alpha=0.11899991337691343, learning_rate=0.08512323674014961, loss='lad', max_depth=1, max_features='auto',\n",
    "    min_impurity_decrease=0.3291504192766261, min_samples_leaf=3, min_samples_split=5, n_estimators=930,\n",
    "    subsample=0.6041295730833111, random_state=42)\n",
    "\n",
    "gb_pipe = Pipeline(steps=[\n",
    "    ('trans',  KeepColumnsTransformer(['month_cos', 'knn_local_knowledge'])),\n",
    "    ('sc', StandardScaler()),\n",
    "    ('gb', gb_model)\n",
    "])\n",
    "# original prediction residuals\n",
    "orig_resids = pd.read_csv('gradient_boosting_predictions_&_residuals_2019')[\n",
    "    'Residuals']\n",
    "# prediction intervals\n",
    "percentiles = []\n",
    "for i in range(len(monthly_test_features)):\n",
    "    print(f'Processing prediction interval for prediction {i+1}')\n",
    "    new_pred_vals = []\n",
    "    for j in range(1000):\n",
    "        bs_resamp = resample(combi_train, n_samples=30)  # bootstrap resample\n",
    "        features = bs_resamp[\n",
    "            ['month_sin', 'month_cos', 'exp_mean_ratio_outsidetemp_gas_used', 'exp_mean_ratio_housetemp_gas_used', 'knn_local_knowledge']]\n",
    "        target = bs_resamp['gas_used_M3']\n",
    "        gb_pipe.fit(features, np.array(target).reshape(-1,))\n",
    "        pred = gb_pipe.predict(monthly_test_features.iloc[i,:])\n",
    "        # check which residual to add, this fit or original fit\n",
    "        rand_resid = np.random.choice(orig_resids)\n",
    "        # add random original residual to prediction and store\n",
    "        new_pred_vals.append(pred + rand_resid)\n",
    "    p = np.percentile(new_pred_vals, q=[2.5, 97.5])  # compute percentiles\n",
    "    percentiles.append(tuple(p))\n",
    "\n",
    "# get percentiles\n",
    "lower_perc = [x for x, _ in percentiles]\n",
    "upper_perc = [x for _, x in percentiles]\n",
    "# load model predictions\n",
    "model_preds = pd.read_csv('gradient_boosting_predictions_&_residuals_2019')[\n",
    "    '2019 Preds']\n",
    "\n",
    "# plot prediction interval\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(range(1, 13), model_preds, color='red', label='predictions')\n",
    "plt.plot(range(1,13), monthly_test_target, color='blue', label='actual')\n",
    "plt.plot(range(1,13), lower_perc, color='orange', label='2.5th percentile', linestyle='--')\n",
    "plt.plot(range(1,13), upper_perc, color='orange', label='97.5th percentile')\n",
    "plt.fill_between(range(1,13), lower_perc, upper_perc, alpha=.1, color='blue')\n",
    "plt.title('Gradient Boosting Prediction Intervals (2.5, 97.5 percentiles)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Gas Used M3')\n",
    "plt.legend(loc='best')\n",
    "plt.hlines(0, 1, 12, color='black', alpha=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:24.521556Z",
     "start_time": "2021-06-04T15:21:24.516472Z"
    }
   },
   "outputs": [],
   "source": [
    "lower_perc # 2.5th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:50.754230Z",
     "start_time": "2021-06-04T15:21:50.736236Z"
    }
   },
   "outputs": [],
   "source": [
    "upper_perc # 97.5th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T15:42:14.110062Z",
     "start_time": "2021-06-07T15:42:14.083040Z"
    }
   },
   "outputs": [],
   "source": [
    "# cost predictions, gradient boosting vs actual\n",
    "preds = pd.read_csv('gradient_boosting_predictions_&_residuals_2019')['2019 Preds']\n",
    "_, monthly_test_target = load_clean_test_data()\n",
    "total_pred_gas = np.round(sum(preds), 2)\n",
    "total_cost_gas = cost_of_gas(total_pred_gas, 2019)\n",
    "total_actual_gas = float(np.round(sum(monthly_test_target.values), 2))\n",
    "total_actual_cost = cost_of_gas(total_actual_gas, 2019)\n",
    "print(f'total pred gas: {total_pred_gas},    total pred cost: {total_cost_gas}')\n",
    "print(f'total actual gas: {total_actual_gas},    total actual cost: {total_actual_cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Neural Network -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T05:43:01.787224Z",
     "start_time": "2021-06-18T05:42:35.344264Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate neural network\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "\n",
    "def nn():\n",
    "    nn_model = Sequential(\n",
    "        [\n",
    "            Input(shape=(2,)),\n",
    "            Dense(units=176, activation='relu', kernel_initializer='glorot_uniform', activity_regularizer=regularizers.l2(l2=0.0015525209418522444)),\n",
    "            Dense(units=176, activation='relu', kernel_initializer='glorot_uniform'),\n",
    "            Dropout(rate=0.34),\n",
    "            Dense(units=400, activation='relu', kernel_initializer='glorot_uniform', activity_regularizer=regularizers.l2(l2=0.0015525209418522444)),\n",
    "            Dense(units=1, activation='relu', kernel_initializer='glorot_uniform')\n",
    "    ])\n",
    "\n",
    "    loss=MeanSquaredError()\n",
    "    opt=Adam(learning_rate=0.001989751755243559, beta_1=0.7500000000000001)\n",
    "    nn_model.compile(optimizer=opt, loss=loss, metrics=[RootMeanSquaredError()])\n",
    "    return nn_model\n",
    "\n",
    "model = KerasRegressor(build_fn=nn, epochs=500, batch_size=12, shuffle=False)\n",
    "nn_pipe = Pipeline(steps=[\n",
    "    ('sc', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2, whiten=True)),\n",
    "    ('model', model)\n",
    "])\n",
    "rmse_scores, mae_scores, mape_scores, r2_scores, _ = expanding_window_cv(monthly_train_features, np.array(monthly_train_target).reshape(-1,), nn_pipe, fold_size=12, init_fold=12)\n",
    "print(f'Neural Network Scores:\\nRMSE: {np.mean(rmse_scores)}\\nMAE: {np.mean(mae_scores)}\\nMAPE: {np.mean(mape_scores)}\\nR2: {np.mean(r2_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T05:22:02.165821Z",
     "start_time": "2021-06-14T05:22:02.158398Z"
    }
   },
   "outputs": [],
   "source": [
    "# neural network for evaluation, tuning\n",
    "def nn_model(hp):\n",
    "    d1 = hp.Int('units1', min_value=16, max_value=512, step=32)\n",
    "    d2 = hp.Int('units2', min_value=16, max_value=512, step=32)\n",
    "    d3 = hp.Int('units3', min_value=16, max_value=512, step=32)\n",
    "    act1 = hp.Choice('activation1', values=['relu','tanh','elu','selu','exponential'])\n",
    "    act2 = hp.Choice('activation2', values=['relu','tanh','elu','selu','exponential'])\n",
    "    act3 = hp.Choice('activation3', values=['relu','tanh','elu','selu','exponential'])\n",
    "    act4 = hp.Choice('activation4', values=['relu','tanh','elu','selu','exponential'])\n",
    "    kern_init = hp.Choice('kernel_initializer', values=['glorot_uniform','glorot_normal','random_normal','random_uniform','truncated_normal','zeros','ones'])\n",
    "    d_rate = hp.Float('drop_rate', min_value=0.05, max_value=0.4, step=0.01)\n",
    "    l_2 = hp.Float('activity_regularizer', min_value=0.0001, max_value=0.01, sampling='log')\n",
    "    l_rate = hp.Float('learning_rate', min_value=0.0001, max_value=0.01, sampling='log')\n",
    "    b1 = hp.Float('beta', min_value=0.6, max_value=0.9, step=0.05)\n",
    "    \n",
    "    nn_model = Sequential(\n",
    "        [\n",
    "            Input(shape=(2,)),\n",
    "            Dense(units=d1, activation=act1, kernel_initializer=kern_init, activity_regularizer=regularizers.l2(l2=l_2)),\n",
    "            Dense(units=d2, activation=act2, kernel_initializer=kern_init),\n",
    "            Dropout(rate=d_rate),\n",
    "            Dense(units=d3, activation=act3, kernel_initializer=kern_init, activity_regularizer=regularizers.l2(l2=l_2)),\n",
    "            Dense(units=1, activation=act4, kernel_initializer=kern_init)\n",
    "    ])\n",
    "\n",
    "    loss=MeanSquaredError()\n",
    "    opt=Adam(learning_rate=l_rate, beta_1=b1)\n",
    "    nn_model.compile(optimizer=opt, loss=loss, metrics=[RootMeanSquaredError()])\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T07:26:04.339858Z",
     "start_time": "2021-06-14T06:08:30.993771Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keras tuner\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "# scale_features\n",
    "sc = StandardScaler()\n",
    "monthly_train_features = sc.fit_transform(monthly_train_features)\n",
    "# apply pca\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "monthly_train_features = pca.fit_transform(monthly_train_features)\n",
    "\n",
    "# tuner = BayesianOptimization(nn_model, objective=kt.Objective('root_mean_squared_error', direction='min'), max_trials=1000, num_initial_points=12, seed=42, directory=os.path.normpath('C:/nn_model_tuning'))\n",
    "# tuner = Hyperband(nn_model, objective=kt.Objective('val_root_mean_squared_error', direction='min'), max_epochs=100, hyperband_iterations=1, seed=42, overwrite=True, directory=os.path.normpath('C:/'))\n",
    "tuner = RandomSearch(nn_model, objective=kt.Objective('val_root_mean_squared_error', direction='min'), max_trials=500, seed=42, overwrite=True, directory=os.path.normpath('C:/'))\n",
    "early_stop = EarlyStopping('val_root_mean_squared_error', patience=20)\n",
    "tuner.search(monthly_train_features, np.array(monthly_train_target).reshape(-1,), epochs=500, callbacks=[early_stop], validation_split=.2)\n",
    "best_hps = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T07:31:00.849920Z",
     "start_time": "2021-06-14T07:31:00.834927Z"
    }
   },
   "outputs": [],
   "source": [
    "units1 = best_hps.get('units1')\n",
    "units2 = best_hps.get('units2')\n",
    "units3 = best_hps.get('units3')\n",
    "activ1 = best_hps.get('activation1')\n",
    "activ2 = best_hps.get('activation2')\n",
    "activ3 = best_hps.get('activation3')\n",
    "activ4 = best_hps.get('activation4')\n",
    "k_init = best_hps.get('kernel_initializer')\n",
    "d_rate = best_hps.get('drop_rate')\n",
    "activ_reg = best_hps.get('activity_regularizer')\n",
    "l_rate = best_hps.get('learning_rate')\n",
    "beta1 = best_hps.get('beta')\n",
    "\n",
    "print('Best Hyperparameters:\\n')\n",
    "print(f'Dense units 1: {units1}\\nDense units 2: {units2}\\nDense units 3: {units3}')\n",
    "print(f'Activation 1: {activ1}\\nActivation 2: {activ2}\\nActivation 3: {activ3}\\nActivation 4: {activ4}')\n",
    "print(f'Kernel initializer: {k_init}\\nDropout rate: {d_rate}')\n",
    "print(f'Activity regularizer: {activ_reg}\\nOpt learning rate: {l_rate}\\nOpt beta 1: {beta1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T06:20:36.521490Z",
     "start_time": "2021-06-09T06:20:36.506490Z"
    }
   },
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:42:16.369129Z",
     "start_time": "2021-06-17T06:40:35.984085Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train neural net\n",
    "# load data\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "monthly_test_features, monthly_test_target = load_clean_test_data()\n",
    "# create neural net\n",
    "nn_model = Sequential(\n",
    "    [\n",
    "        Input(shape=(2,)),\n",
    "        Dense(units=176, activation='selu', kernel_initializer='truncated_normal', activity_regularizer=regularizers.l2(l2=0.0015525209418522444)),\n",
    "        Dense(units=176, activation='relu', kernel_initializer='truncated_normal'),\n",
    "        Dropout(rate=0.34),\n",
    "        Dense(units=400, activation='exponential', kernel_initializer='truncated_normal', activity_regularizer=regularizers.l2(l2=0.0015525209418522444)),\n",
    "        Dense(units=1, activation='elu', kernel_initializer='truncated_normal')\n",
    "])\n",
    "\n",
    "loss=MeanSquaredError()\n",
    "opt=Adam(learning_rate=0.001989751755243559, beta_1=0.7500000000000001)\n",
    "nn_model.compile(optimizer=opt, loss=loss, metrics=[RootMeanSquaredError()])\n",
    "\n",
    "# set up train & validation data.. validation data is year 2018\n",
    "#monthly_train_features = monthly_train_features.iloc[:-12,:]\n",
    "#monthly_train_target = monthly_train_target[:-12]\n",
    "#monthly_val_features = monthly_train_features.iloc[-12:,:]\n",
    "#monthly_val_target = monthly_train_target[-12:]\n",
    "\n",
    "# data pipeline\n",
    "sc = StandardScaler()\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "# fit scaler and pca on train data\n",
    "sc.fit(monthly_train_features)\n",
    "pca.fit(monthly_train_features)\n",
    "monthly_train_features = sc.transform(monthly_train_features)\n",
    "monthly_train_features = pca.transform(monthly_train_features)\n",
    "# transform validation data with scaler and pca\n",
    "monthly_test_features = sc.transform(monthly_test_features)\n",
    "monthly_test_features = pca.transform(monthly_test_features)\n",
    "\n",
    "# save scaler and pca\n",
    "dump(sc, open('nn_scaler.pkl', 'wb')) ### to load later use... sc = load(open('nn_scaler.pkl', 'rb'))\n",
    "dump(pca, open('nn_pca.pkl', 'wb')) ### to load later use... pca = load(open('nn_pca.pkl', 'rb'))\n",
    "\n",
    "early_stop = EarlyStopping('val_root_mean_squared_error', patience=200)\n",
    "######## saves full model #########  to load model... load_model('path/to/location')\n",
    "checkpoint = ModelCheckpoint(filepath='neural_net_model.ckpt', monitor='val_root_mean_squared_error', save_weights_only=False, save_best_only=True, mode='min')\n",
    "# fit neural net\n",
    "nn_model.fit(monthly_train_features, np.array(monthly_train_target).reshape(-1,), epochs=10000, validation_data=(monthly_test_features, np.array(monthly_test_target).reshape(-1,)), callbacks=[early_stop, checkpoint], shuffle=False, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T06:43:07.873668Z",
     "start_time": "2021-06-17T06:43:06.845128Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict and plot test data\n",
    "# load data, scaler, pca and model\n",
    "monthly_test_features, monthly_test_target = load_clean_test_data()\n",
    "sc = load(open('nn_scaler.pkl', 'rb'))\n",
    "pca = load(open('nn_pca.pkl', 'rb'))\n",
    "nn = load_model('neural_net_model.ckpt')\n",
    "# apply scaler and pca transforms\n",
    "monthly_test_features = sc.transform(monthly_test_features)\n",
    "monthly_test_features = pca.transform(monthly_test_features)\n",
    "# get predictions and rmse\n",
    "preds = nn.predict(monthly_test_features)\n",
    "rmse = mean_squared_error(monthly_test_target, preds, squared=False)\n",
    "print(f'Neural Net RMSE: {rmse}')\n",
    "# cost of predicted gas compared to actual gas usage\n",
    "total_pred_gas = np.round(float(sum(preds)),2)\n",
    "total_cost_gas = cost_of_gas(total_pred_gas, 2019)\n",
    "total_actual_gas = float(np.round(sum(monthly_test_target.values), 2))\n",
    "total_actual_cost = cost_of_gas(total_actual_gas, 2019)\n",
    "print(f'total pred gas: {total_pred_gas},    total pred cost: {total_cost_gas}')\n",
    "print(f'total actual gas: {total_actual_gas},    total actual cost: {total_actual_cost}\\n')\n",
    "# plot pred vs actual\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(range(1,13), preds, label='predictions', color='blue')\n",
    "plt.plot(range(1,13), monthly_test_target, label='actual', color='red')\n",
    "plt.title('Neural Net predictions vs actual gas usage')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Gas Used M3')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T05:11:09.616851Z",
     "start_time": "2021-06-23T05:11:09.608852Z"
    }
   },
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(features, target, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(features)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(features):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = features[i:end_ix, :], target[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T05:20:41.520546Z",
     "start_time": "2021-06-23T05:20:41.509297Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_model():\n",
    "    model = Sequential([\n",
    "        LSTM(units=64, activation='relu',\n",
    "             return_sequences=True, input_shape=(12, 2), dropout=0.1, recurrent_dropout=0.1),\n",
    "        LSTM(units=32, activation='relu'),\n",
    "        Dense(units=12)\n",
    "    ])\n",
    "    loss = MeanSquaredError()\n",
    "    opt = Adam()\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T05:31:57.952732Z",
     "start_time": "2021-06-23T05:21:11.116748Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "monthly_train_features, monthly_train_target = load_clean_train_data()\n",
    "monthly_test_features, monthly_test_target = load_clean_test_data()\n",
    "# load scaler and pca\n",
    "sc = load(open('nn_scaler.pkl', 'rb'))\n",
    "pca = load(open('nn_pca.pkl', 'rb'))\n",
    "# transform train and test data\n",
    "monthly_train_features = sc.transform(monthly_train_features)\n",
    "monthly_train_features = pca.transform(monthly_train_features)\n",
    "monthly_test_features = sc.transform(monthly_test_features)\n",
    "monthly_test_features = pca.transform(monthly_test_features)\n",
    "#monthly_test_features = monthly_test_features.reshape(-1,12,2)\n",
    "# split train data\n",
    "train_features, train_target = split_sequences(monthly_train_features, monthly_train_target, 12, 12)\n",
    "# convert to features tensor\n",
    "#train_features = tf.convert_to_tensor(train_features, dtype=tf.float32)\n",
    "# load model\n",
    "model = lstm_model()\n",
    "# instantiate callbacks\n",
    "early_stop = EarlyStopping('root_mean_squared_error', patience=100)\n",
    "######## saves full model #########  to load model... load_model('path/to/location')\n",
    "checkpoint = ModelCheckpoint(filepath='lstm_model.ckpt', monitor='root_mean_squared_error', save_weights_only=False, save_best_only=True, mode='min')\n",
    "\n",
    "model.fit(train_features, train_target, epochs=10000, callbacks=[early_stop, checkpoint], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T05:37:40.162712Z",
     "start_time": "2021-06-23T05:37:33.552842Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict and plot test data\n",
    "# load data, scaler, pca and model\n",
    "monthly_test_features, monthly_test_target = load_clean_test_data()\n",
    "sc = load(open('nn_scaler.pkl', 'rb'))\n",
    "pca = load(open('nn_pca.pkl', 'rb'))\n",
    "lstm_model = load_model('lstm_model.ckpt')\n",
    "# apply scaler and pca transforms\n",
    "monthly_test_features = sc.transform(monthly_test_features)\n",
    "monthly_test_features = pca.transform(monthly_test_features)\n",
    "monthly_test_features = monthly_test_features.reshape(-1,12,2)\n",
    "# get predictions and rmse\n",
    "preds = lstm_model.predict(monthly_test_features).flatten()\n",
    "rmse = mean_squared_error(monthly_test_target, preds, squared=False)\n",
    "print(f'LSTM Model RMSE: {rmse}')\n",
    "# cost of predicted gas compared to actual gas usage\n",
    "total_pred_gas = np.round(float(sum(preds)),2)\n",
    "total_cost_gas = cost_of_gas(total_pred_gas, 2019)\n",
    "total_actual_gas = float(np.round(sum(monthly_test_target.values), 2))\n",
    "total_actual_cost = cost_of_gas(total_actual_gas, 2019)\n",
    "print(f'total pred gas: {total_pred_gas},    total pred cost: {total_cost_gas}')\n",
    "print(f'total actual gas: {total_actual_gas},    total actual cost: {total_actual_cost}\\n')\n",
    "# plot pred vs actual\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(range(1,13), preds, label='predictions', color='blue')\n",
    "plt.plot(range(1,13), monthly_test_target, label='actual', color='red')\n",
    "plt.title('LSTM Model predictions vs actual gas usage')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Gas Used M3')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Time Series Forecast --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data Preperation -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:07:27.597918Z",
     "start_time": "2021-09-23T08:07:26.940676Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "gas_consumption = pd.read_csv('gas_consumption.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:07:27.672161Z",
     "start_time": "2021-09-23T08:07:27.597918Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove last months of 2013 and early months of 2020\n",
    "# split train/test\n",
    "gc_train = gas_consumption[(gas_consumption['date_year'] > 2013) & (\n",
    "    gas_consumption['date_year'] < 2019)]\n",
    "gc_test = gas_consumption[(gas_consumption['date_year'] > 2018) & (\n",
    "    gas_consumption['date_year'] < 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:07:27.875560Z",
     "start_time": "2021-09-23T08:07:27.672161Z"
    }
   },
   "outputs": [],
   "source": [
    "# obtain monthly consumption of gas per year -- train data\n",
    "gc_monthly = gc_train.groupby(by=['date_year', 'date_month']).agg({\n",
    "    'gas_consumed_01_M3': ['min', 'max']})\n",
    "gc_monthly['gas_used'] = gc_monthly['gas_consumed_01_M3']['max'] - \\\n",
    "    gc_monthly['gas_consumed_01_M3']['min']\n",
    "gc_monthly['gas_used_M3'] = gc_monthly['gas_used'] * 0.1\n",
    "gc_monthly.reset_index(inplace=True)\n",
    "month_date = []\n",
    "for i in range(len(gc_monthly)):\n",
    "    yr = gc_monthly['date_year'][i]\n",
    "    mnth = gc_monthly['date_month'][i]\n",
    "    date_str = f'{yr}-{mnth}-01'\n",
    "    month_date.append(date_str)\n",
    "gc_monthly['month_date'] = month_date\n",
    "gc_monthly['month_date'] = pd.to_datetime(gc_monthly['month_date'])\n",
    "gc_monthly.index = gc_monthly['month_date']\n",
    "gc_monthly.drop(columns=['date_year', 'date_month',\n",
    "                'month_date'], level=0, inplace=True)\n",
    "gc_monthly = gc_monthly[['gas_used_M3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:07:27.967985Z",
     "start_time": "2021-09-23T08:07:27.906676Z"
    }
   },
   "outputs": [],
   "source": [
    "# testing data\n",
    "# future monthly data to predict\n",
    "gc_future_monthly = gc_test.groupby(by=['date_year', 'date_month']).agg({\n",
    "    'gas_consumed_01_M3': ['min', 'max']})\n",
    "gc_future_monthly['gas_used'] = gc_future_monthly['gas_consumed_01_M3']['max'] - \\\n",
    "    gc_future_monthly['gas_consumed_01_M3']['min']\n",
    "gc_future_monthly['gas_used_M3'] = gc_future_monthly['gas_used'] * 0.1\n",
    "# change the index to month datetime\n",
    "gc_future_monthly.reset_index(inplace=True)\n",
    "month_date = []\n",
    "for i in range(len(gc_future_monthly)):\n",
    "    yr = gc_future_monthly['date_year'][i]\n",
    "    mnth = gc_future_monthly['date_month'][i]\n",
    "    date_str = f'{yr}-{mnth}-01'\n",
    "    month_date.append(date_str)\n",
    "gc_future_monthly['month_date'] = month_date\n",
    "gc_future_monthly['month_date'] = pd.to_datetime(\n",
    "    gc_future_monthly['month_date'])\n",
    "gc_future_monthly.index = gc_future_monthly['month_date']\n",
    "gc_future_monthly.drop(\n",
    "    columns=['date_year', 'date_month', 'month_date'], level=0, inplace=True)\n",
    "gc_future_monthly = gc_future_monthly[['gas_used_M3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - EDA -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T09:52:10.528758Z",
     "start_time": "2021-09-22T09:52:10.512798Z"
    }
   },
   "outputs": [],
   "source": [
    "gc_monthly.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T12:06:35.093461Z",
     "start_time": "2021-09-22T12:06:34.954467Z"
    }
   },
   "outputs": [],
   "source": [
    "# autocorrelation plot\n",
    "pd.plotting.autocorrelation_plot(gc_monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T09:54:02.742899Z",
     "start_time": "2021-09-22T09:54:02.543891Z"
    }
   },
   "outputs": [],
   "source": [
    "gc_monthly.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on first inspection trend looks fairly constant and annual seasonality is apparent. The amplitude of the cycles also seem fairly consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T10:09:33.372779Z",
     "start_time": "2021-09-22T10:09:33.343737Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check mean & variance of splits from train data -- initial stationary test\n",
    "split = np.int(len(gc_monthly) / 2)\n",
    "split1, split2 = gc_monthly[:split], gc_monthly[split:]\n",
    "mean1, mean2 = split1.mean(), split2.mean()\n",
    "var1, var2 = split1.var(), split2.var()\n",
    "print(f'mean 1: {mean1}, mean 2: {mean2}')\n",
    "print(f'var 1: {var1}, var 2: {var2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initial stationary test appears to show relative stationarity with means and variances fairly consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T10:13:41.040436Z",
     "start_time": "2021-09-22T10:13:40.914399Z"
    }
   },
   "outputs": [],
   "source": [
    "gc_monthly.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T08:07:35.728152Z",
     "start_time": "2021-09-23T08:07:35.138574Z"
    }
   },
   "outputs": [],
   "source": [
    "# seasonal decomposition\n",
    "decomp = seasonal_decompose(gc_monthly, model='additive')\n",
    "decomp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T10:20:55.516600Z",
     "start_time": "2021-09-22T10:20:55.491574Z"
    }
   },
   "outputs": [],
   "source": [
    "# test for stationarity\n",
    "print('Results of Dickey-Fuller Test:')\n",
    "st_test = adfuller(gc_monthly['gas_used_M3'], maxlag=12, autolag='AIC')\n",
    "st_output = pd.Series(st_test[0:4], index=[\n",
    "                     'Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in st_test[4].items():\n",
    "    st_output['Critical Value (%s)' % key] = value\n",
    "print(st_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test statistic is lower than the 1% critical value we can say with 99% confidence that the monthly gas consumption data is stationary, also the p-value is well below the significance level meaning we are not likely to have seen this by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonal Shifted Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T10:54:20.449229Z",
     "start_time": "2021-09-22T10:54:19.964266Z"
    }
   },
   "outputs": [],
   "source": [
    "gc_shft_12 = gc_monthly['gas_used_M3'] - gc_monthly['gas_used_M3'].shift(12)\n",
    "gc_shft_12 = gc_shft_12.dropna()\n",
    "shft_decomp = seasonal_decompose(gc_shft_12, model='additive')\n",
    "shft_decomp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T10:59:54.520488Z",
     "start_time": "2021-09-22T10:59:54.492477Z"
    }
   },
   "outputs": [],
   "source": [
    "# test for stationarity on shifted data\n",
    "print('Results of Dickey-Fuller Test:')\n",
    "st_test = adfuller(gc_shft_12, maxlag=12, autolag='AIC')\n",
    "st_output = pd.Series(st_test[0:4], index=[\n",
    "                     'Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in st_test[4].items():\n",
    "    st_output['Critical Value (%s)' % key] = value\n",
    "print(st_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the seasonally shifted data still appears to be stationary, the test statitistic after seasonal shifted decomposition is just below the 1% critical value and the p-value is a lot higher than the original decomposition, meaning the original has better stationarity than the seasonally shifted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T11:04:14.441552Z",
     "start_time": "2021-09-22T11:04:13.923541Z"
    }
   },
   "outputs": [],
   "source": [
    "gc_shft_1 = gc_monthly['gas_used_M3'] - gc_monthly['gas_used_M3'].shift(1)\n",
    "gc_shft_1 = gc_shft_1.dropna()\n",
    "shft_decomp = seasonal_decompose(gc_shft_1, model='additive')\n",
    "shft_decomp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T11:04:49.037930Z",
     "start_time": "2021-09-22T11:04:49.019915Z"
    }
   },
   "outputs": [],
   "source": [
    "# test for stationarity on shifted data\n",
    "print('Results of Dickey-Fuller Test:')\n",
    "st_test = adfuller(gc_shft_1, maxlag=12, autolag='AIC')\n",
    "st_output = pd.Series(st_test[0:4], index=[\n",
    "                     'Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in st_test[4].items():\n",
    "    st_output['Critical Value (%s)' % key] = value\n",
    "print(st_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Baseline -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T11:44:08.386224Z",
     "start_time": "2021-09-22T11:44:08.187219Z"
    }
   },
   "outputs": [],
   "source": [
    "# use the average of monthly averages for each month\n",
    "avg_monthly = gc_monthly.copy()\n",
    "avg_monthly.reset_index(inplace=True)\n",
    "total_avg_gas = []\n",
    "for i in range(12):\n",
    "    mnth_idx = avg_monthly['month_date'].dt.month == i+1\n",
    "    avg_mnth_gas = np.round(\n",
    "        np.mean(avg_monthly[mnth_idx]['gas_used_M3'].values), 2)\n",
    "    total_avg_gas.append(avg_mnth_gas)\n",
    "\n",
    "rmse = mean_squared_error(gc_future_monthly, total_avg_gas, squared=False)\n",
    "\n",
    "print(\n",
    "    f'Monthly average baseline RMSE score: {rmse}')\n",
    "\n",
    "# plot of average gas forecast\n",
    "gc_future_pred = pd.DataFrame(\n",
    "    {'avg_pred': total_avg_gas}, index=gc_future_monthly.index)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(gc_future_monthly.index,\n",
    "         gc_future_monthly['gas_used_M3'].values, color='blue', label='actual')\n",
    "plt.plot(gc_future_pred['avg_pred'], color='red', label='monthly average pred')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Monthly Gas Forecast Using Monthly Averages')\n",
    "plt.xlabel('Forecast Horizon (Months)')\n",
    "plt.ylabel('Gas Used (M3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Modelling -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T09:42:56.481879Z",
     "start_time": "2021-09-23T09:42:56.347909Z"
    }
   },
   "outputs": [],
   "source": [
    "# model seasonality\n",
    "X = [i%12 for i in range(0, len(gc_monthly))]\n",
    "y = gc_monthly.values.flatten()\n",
    "degree = 9\n",
    "coef = np.polyfit(X, y, degree)\n",
    "print(f'Coefficients: {coef}')\n",
    "# create curve\n",
    "curve = list()\n",
    "for i in range(len(X)):\n",
    "    value = coef[-1]\n",
    "    for d in range(degree):\n",
    "        value += X[i]**(degree-d) * coef[d]\n",
    "    curve.append(value)\n",
    "# plot curve over original data\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(gc_monthly.values)\n",
    "plt.plot(curve, color='red', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:27:54.201563Z",
     "start_time": "2021-09-23T13:27:54.035549Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_model = pd.Series(curve[:12], index=gc_future_monthly.index)\n",
    "rmse = mean_squared_error(gc_future_monthly.values, data_model.values, squared=False)\n",
    "r2 = r2_score(gc_future_monthly.values, data_model.values)\n",
    "print(f'Poly Model RMSE: {rmse},   R2: {r2}')\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(gc_future_monthly, label='actual', color='blue')\n",
    "plt.plot(data_model, label='model', color='red')\n",
    "plt.title('Polynomial Model vs Actual Data')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Gas USed M3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:57:01.939368Z",
     "start_time": "2021-09-23T13:57:01.435661Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arima = ARIMA(gc_monthly, order=(2,0,5), seasonal_order=(0,0,0,12))\n",
    "arima_fit = arima.fit()\n",
    "print(arima_fit.summary())\n",
    "# line plot of residuals\n",
    "resids = pd.DataFrame(arima_fit.resid)\n",
    "resids.plot()\n",
    "plt.show()\n",
    "# density plot of residuals\n",
    "resids.plot(kind='kde')\n",
    "plt.show()\n",
    "# summary stats of residuals\n",
    "print(resids.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:57:10.830796Z",
     "start_time": "2021-09-23T13:57:10.335685Z"
    }
   },
   "outputs": [],
   "source": [
    "arima = ARIMA(gc_monthly, order=(2,0,5), seasonal_order=(0,0,0,12))\n",
    "arima_fit = arima.fit()\n",
    "arima_preds = arima_fit.predict(start=len(gc_monthly)+1, end=len(gc_monthly)+12, typ='levels')\n",
    "# evaluate forecasts\n",
    "rmse = mean_squared_error(gc_future_monthly.values, arima_preds, squared=False)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# plot forecasts against actual outcomes\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,13), gc_future_monthly.values, label='actual', color='blue')\n",
    "plt.plot(range(1,13), arima_preds, label='predicted', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T10:03:32.276087Z",
     "start_time": "2021-09-23T10:03:29.429269Z"
    }
   },
   "outputs": [],
   "source": [
    "# prophet forecast\n",
    "# train data\n",
    "train_data = pd.DataFrame(\n",
    "    {'gas_used_M3': gc_monthly['gas_used_M3'].values}, index=gc_monthly.index)\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data.columns = ['ds', 'y']\n",
    "# test data\n",
    "test_data = gc_future_monthly.reset_index()\n",
    "test_data = test_data[['month_date']]\n",
    "test_data.columns = ['ds']\n",
    "\n",
    "model = Prophet(changepoint_prior_scale=0.001, seasonality_prior_scale=0.1)\n",
    "model.fit(train_data)\n",
    "forecast = model.predict(test_data)\n",
    "actual = gc_future_monthly['gas_used_M3'].values\n",
    "predicted = forecast['yhat'].values\n",
    "predicted_upper = forecast['yhat_upper'].values\n",
    "predicted_lower = forecast['yhat_lower'].values\n",
    "\n",
    "proph_rmse = mean_squared_error(actual, predicted, squared=False)\n",
    "proph_r2 = r2_score(actual, predicted)\n",
    "print(\n",
    "    f'Prophet forecast RMSE score: {proph_rmse},    R2 score: {proph_r2}')\n",
    "\n",
    "pred_df = pd.DataFrame({'pred': predicted}, index=gc_future_monthly.index)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(gc_future_monthly.index,\n",
    "         gc_future_monthly['gas_used_M3'], color='blue', label='actual')\n",
    "plt.plot(pred_df, color='red', label='prophet forecast')\n",
    "#plt.fill_between(pred_df['pred'], predicted_lower, predicted_upper, color='red', alpha=.2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Prophet Forecast of HouseHold Gas Usage')\n",
    "plt.xlabel('Forecast Horizon (Months)')\n",
    "plt.ylabel('Gas Used (M3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T16:11:28.474433Z",
     "start_time": "2021-09-22T15:41:10.707421Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prophet hyperparameter tuning\n",
    "\n",
    "param_grid = {  \n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
    "rmses = []  # Store the RMSEs for each params here\n",
    "\n",
    "# Use cross validation to evaluate all parameters\n",
    "for params in all_params:\n",
    "    model = Prophet(**params).fit(train_data)  # Fit model with given params\n",
    "    model_cv = cross_validation(model, period=12, horizon=12, parallel=\"processes\")\n",
    "    model_p = performance_metrics(model_cv, rolling_window=1)\n",
    "    rmses.append(model_p['rmse'].values[0])\n",
    "\n",
    "# Find the best parameters\n",
    "#tuning_results = pd.DataFrame(all_params)\n",
    "#tuning_results['rmse'] = rmses\n",
    "#print(tuning_results)\n",
    "\n",
    "best_params = all_params[np.argmin(rmses)]\n",
    "print(f'Top parameters:\\n{best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T16:13:26.308703Z",
     "start_time": "2021-09-22T16:13:26.291689Z"
    }
   },
   "outputs": [],
   "source": [
    "tuning_results = pd.DataFrame(all_params)\n",
    "tuning_results['rmse'] = rmses\n",
    "tuning_results.sort_values(by='rmse').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Final Models Plot -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T13:36:50.834563Z",
     "start_time": "2021-09-23T13:36:50.657547Z"
    }
   },
   "outputs": [],
   "source": [
    "# final time series plot\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,13), data_model.values, label='poly prediction', color='red')\n",
    "plt.plot(range(1,13), arima_preds, label='arima prediction', color='orange')\n",
    "plt.plot(range(1,13), pred_df.values, label='prophet prediction', color='green')\n",
    "plt.plot(range(1,13), gc_future_monthly.values, label='actual', color='blue')\n",
    "plt.title('Time Series Model Predictions vs Actual')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Gas Used M3')\n",
    "plt.legend()\n",
    "plt.grid(alpha=.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
